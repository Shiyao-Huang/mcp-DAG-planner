#!/usr/bin/env python3
"""
MCP Feedback Enhanced ä¼ºæœå™¨ä¸»è¦æ¨¡çµ„

æ­¤æ¨¡çµ„æä¾› MCP (Model Context Protocol) çš„å¢å¼·å›é¥‹æ”¶é›†åŠŸèƒ½ï¼Œ
æ”¯æ´æ™ºèƒ½ç’°å¢ƒæª¢æ¸¬ï¼Œè‡ªå‹•ä½¿ç”¨ Web UI ä»‹é¢ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
- MCP å·¥å…·å¯¦ç¾
- ä»‹é¢é¸æ“‡ï¼ˆWeb UIï¼‰
- ç’°å¢ƒæª¢æ¸¬ (SSH Remote, WSL, Local)
- åœ‹éš›åŒ–æ”¯æ´
- åœ–ç‰‡è™•ç†èˆ‡ä¸Šå‚³
- å‘½ä»¤åŸ·è¡Œèˆ‡çµæœå±•ç¤º
- å°ˆæ¡ˆç›®éŒ„ç®¡ç†

ä¸»è¦ MCP å·¥å…·ï¼š
- interactive_feedback: æ”¶é›†ç”¨æˆ¶äº’å‹•å›é¥‹
- get_system_info: ç²å–ç³»çµ±ç’°å¢ƒè³‡è¨Š

ä½œè€…: FÃ¡bio Ferreira (åŸä½œè€…)
å¢å¼·: Minidoracat (Web UI, åœ–ç‰‡æ”¯æ´, ç’°å¢ƒæª¢æ¸¬)
é‡æ§‹: æ¨¡å¡ŠåŒ–è¨­è¨ˆ
"""

import base64
import datetime
import io
import json
import os
import sys
import asyncio
from pathlib import Path
from typing import Annotated, Any

from fastmcp import FastMCP
from fastmcp.utilities.types import Image as MCPImage
from mcp.types import TextContent
from pydantic import Field

# å°å…¥çµ±ä¸€çš„èª¿è©¦åŠŸèƒ½
from .debug import server_debug_log as debug_log

# å°å…¥å¤šèªç³»æ”¯æ´
# å°å…¥éŒ¯èª¤è™•ç†æ¡†æ¶
from .utils.error_handler import ErrorHandler, ErrorType

# å°å…¥è³‡æºç®¡ç†å™¨
from .utils.resource_manager import create_temp_file


# ===== ç·¨ç¢¼åˆå§‹åŒ– =====
def init_encoding():
    """åˆå§‹åŒ–ç·¨ç¢¼è¨­ç½®ï¼Œç¢ºä¿æ­£ç¢ºè™•ç†ä¸­æ–‡å­—ç¬¦"""
    try:
        # Windows ç‰¹æ®Šè™•ç†
        if sys.platform == "win32":
            import msvcrt

            # è¨­ç½®ç‚ºäºŒé€²åˆ¶æ¨¡å¼
            msvcrt.setmode(sys.stdin.fileno(), os.O_BINARY)
            msvcrt.setmode(sys.stdout.fileno(), os.O_BINARY)

            # é‡æ–°åŒ…è£ç‚º UTF-8 æ–‡æœ¬æµï¼Œä¸¦ç¦ç”¨ç·©è¡
            # ä¿®å¾© union-attr éŒ¯èª¤ - å®‰å…¨ç²å– buffer æˆ– detach
            stdin_buffer = getattr(sys.stdin, "buffer", None)
            if stdin_buffer is None and hasattr(sys.stdin, "detach"):
                stdin_buffer = sys.stdin.detach()

            stdout_buffer = getattr(sys.stdout, "buffer", None)
            if stdout_buffer is None and hasattr(sys.stdout, "detach"):
                stdout_buffer = sys.stdout.detach()

            sys.stdin = io.TextIOWrapper(
                stdin_buffer, encoding="utf-8", errors="replace", newline=None
            )
            sys.stdout = io.TextIOWrapper(
                stdout_buffer,
                encoding="utf-8",
                errors="replace",
                newline="",
                write_through=True,  # é—œéµï¼šç¦ç”¨å¯«å…¥ç·©è¡
            )
        else:
            # é Windows ç³»çµ±çš„æ¨™æº–è¨­ç½®
            if hasattr(sys.stdout, "reconfigure"):
                sys.stdout.reconfigure(encoding="utf-8", errors="replace")
            if hasattr(sys.stdin, "reconfigure"):
                sys.stdin.reconfigure(encoding="utf-8", errors="replace")

        # è¨­ç½® stderr ç·¨ç¢¼ï¼ˆç”¨æ–¼èª¿è©¦è¨Šæ¯ï¼‰
        if hasattr(sys.stderr, "reconfigure"):
            sys.stderr.reconfigure(encoding="utf-8", errors="replace")

        return True
    except Exception:
        # å¦‚æœç·¨ç¢¼è¨­ç½®å¤±æ•—ï¼Œå˜—è©¦åŸºæœ¬è¨­ç½®
        try:
            if hasattr(sys.stdout, "reconfigure"):
                sys.stdout.reconfigure(encoding="utf-8", errors="replace")
            if hasattr(sys.stdin, "reconfigure"):
                sys.stdin.reconfigure(encoding="utf-8", errors="replace")
            if hasattr(sys.stderr, "reconfigure"):
                sys.stderr.reconfigure(encoding="utf-8", errors="replace")
        except:
            pass
        return False


# åˆå§‹åŒ–ç·¨ç¢¼ï¼ˆåœ¨å°å…¥æ™‚å°±åŸ·è¡Œï¼‰
_encoding_initialized = init_encoding()

# ===== å¸¸æ•¸å®šç¾© =====
SERVER_NAME = "äº’å‹•å¼å›é¥‹æ”¶é›† MCP"
SSH_ENV_VARS = ["SSH_CONNECTION", "SSH_CLIENT", "SSH_TTY"]
REMOTE_ENV_VARS = ["REMOTE_CONTAINERS", "CODESPACES"]


# åˆå§‹åŒ– MCP æœå‹™å™¨
from . import __version__


# ç¢ºä¿ log_level è¨­å®šç‚ºæ­£ç¢ºçš„å¤§å¯«æ ¼å¼
fastmcp_settings = {}

# æª¢æŸ¥ç’°å¢ƒè®Šæ•¸ä¸¦è¨­å®šæ­£ç¢ºçš„ log_level
env_log_level = os.getenv("FASTMCP_LOG_LEVEL", "").upper()
if env_log_level in ("DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"):
    fastmcp_settings["log_level"] = env_log_level
else:
    # é è¨­ä½¿ç”¨ INFO ç­‰ç´š
    fastmcp_settings["log_level"] = "INFO"

mcp: Any = FastMCP(SERVER_NAME)


# ===== å·¥å…·å‡½æ•¸ =====
def is_wsl_environment() -> bool:
    """
    æª¢æ¸¬æ˜¯å¦åœ¨ WSL (Windows Subsystem for Linux) ç’°å¢ƒä¸­é‹è¡Œ

    Returns:
        bool: True è¡¨ç¤º WSL ç’°å¢ƒï¼ŒFalse è¡¨ç¤ºå…¶ä»–ç’°å¢ƒ
    """
    try:
        # æª¢æŸ¥ /proc/version æ–‡ä»¶æ˜¯å¦åŒ…å« WSL æ¨™è­˜
        if os.path.exists("/proc/version"):
            with open("/proc/version") as f:
                version_info = f.read().lower()
                if "microsoft" in version_info or "wsl" in version_info:
                    debug_log("åµæ¸¬åˆ° WSL ç’°å¢ƒï¼ˆé€šé /proc/versionï¼‰")
                    return True

        # æª¢æŸ¥ WSL ç›¸é—œç’°å¢ƒè®Šæ•¸
        wsl_env_vars = ["WSL_DISTRO_NAME", "WSL_INTEROP", "WSLENV"]
        for env_var in wsl_env_vars:
            if os.getenv(env_var):
                debug_log(f"åµæ¸¬åˆ° WSL ç’°å¢ƒè®Šæ•¸: {env_var}")
                return True

        # æª¢æŸ¥æ˜¯å¦å­˜åœ¨ WSL ç‰¹æœ‰çš„è·¯å¾‘
        wsl_paths = ["/mnt/c", "/mnt/d", "/proc/sys/fs/binfmt_misc/WSLInterop"]
        for path in wsl_paths:
            if os.path.exists(path):
                debug_log(f"åµæ¸¬åˆ° WSL ç‰¹æœ‰è·¯å¾‘: {path}")
                return True

    except Exception as e:
        debug_log(f"WSL æª¢æ¸¬éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")

    return False


def is_remote_environment() -> bool:
    """
    æª¢æ¸¬æ˜¯å¦åœ¨é ç«¯ç’°å¢ƒä¸­é‹è¡Œ

    Returns:
        bool: True è¡¨ç¤ºé ç«¯ç’°å¢ƒï¼ŒFalse è¡¨ç¤ºæœ¬åœ°ç’°å¢ƒ
    """
    # WSL ä¸æ‡‰è¢«è¦–ç‚ºé ç«¯ç’°å¢ƒï¼Œå› ç‚ºå®ƒå¯ä»¥è¨ªå• Windows ç€è¦½å™¨
    if is_wsl_environment():
        debug_log("WSL ç’°å¢ƒä¸è¢«è¦–ç‚ºé ç«¯ç’°å¢ƒ")
        return False

    # æª¢æŸ¥ SSH é€£ç·šæŒ‡æ¨™
    for env_var in SSH_ENV_VARS:
        if os.getenv(env_var):
            debug_log(f"åµæ¸¬åˆ° SSH ç’°å¢ƒè®Šæ•¸: {env_var}")
            return True

    # æª¢æŸ¥é ç«¯é–‹ç™¼ç’°å¢ƒ
    for env_var in REMOTE_ENV_VARS:
        if os.getenv(env_var):
            debug_log(f"åµæ¸¬åˆ°é ç«¯é–‹ç™¼ç’°å¢ƒ: {env_var}")
            return True

    # æª¢æŸ¥ Docker å®¹å™¨
    if os.path.exists("/.dockerenv"):
        debug_log("åµæ¸¬åˆ° Docker å®¹å™¨ç’°å¢ƒ")
        return True

    # Windows é ç«¯æ¡Œé¢æª¢æŸ¥
    if sys.platform == "win32":
        session_name = os.getenv("SESSIONNAME", "")
        if session_name and "RDP" in session_name:
            debug_log(f"åµæ¸¬åˆ° Windows é ç«¯æ¡Œé¢: {session_name}")
            return True

    # Linux ç„¡é¡¯ç¤ºç’°å¢ƒæª¢æŸ¥ï¼ˆä½†æ’é™¤ WSLï¼‰
    if (
        sys.platform.startswith("linux")
        and not os.getenv("DISPLAY")
        and not is_wsl_environment()
    ):
        debug_log("åµæ¸¬åˆ° Linux ç„¡é¡¯ç¤ºç’°å¢ƒ")
        return True

    return False


def save_feedback_to_file(feedback_data: dict, file_path: str | None = None) -> str:
    """
    å°‡å›é¥‹è³‡æ–™å„²å­˜åˆ° JSON æ–‡ä»¶

    Args:
        feedback_data: å›é¥‹è³‡æ–™å­—å…¸
        file_path: å„²å­˜è·¯å¾‘ï¼Œè‹¥ç‚º None å‰‡è‡ªå‹•ç”¢ç”Ÿè‡¨æ™‚æ–‡ä»¶

    Returns:
        str: å„²å­˜çš„æ–‡ä»¶è·¯å¾‘
    """
    if file_path is None:
        # ä½¿ç”¨è³‡æºç®¡ç†å™¨å‰µå»ºè‡¨æ™‚æ–‡ä»¶
        file_path = create_temp_file(suffix=".json", prefix="feedback_")

    # ç¢ºä¿ç›®éŒ„å­˜åœ¨
    directory = os.path.dirname(file_path)
    if directory and not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)

    # è¤‡è£½æ•¸æ“šä»¥é¿å…ä¿®æ”¹åŸå§‹æ•¸æ“š
    json_data = feedback_data.copy()

    # è™•ç†åœ–ç‰‡æ•¸æ“šï¼šå°‡ bytes è½‰æ›ç‚º base64 å­—ç¬¦ä¸²ä»¥ä¾¿ JSON åºåˆ—åŒ–
    if "images" in json_data and isinstance(json_data["images"], list):
        processed_images = []
        for img in json_data["images"]:
            if isinstance(img, dict) and "data" in img:
                processed_img = img.copy()
                # å¦‚æœ data æ˜¯ bytesï¼Œè½‰æ›ç‚º base64 å­—ç¬¦ä¸²
                if isinstance(img["data"], bytes):
                    processed_img["data"] = base64.b64encode(img["data"]).decode(
                        "utf-8"
                    )
                    processed_img["data_type"] = "base64"
                processed_images.append(processed_img)
            else:
                processed_images.append(img)
        json_data["images"] = processed_images

    # å„²å­˜è³‡æ–™
    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(json_data, f, ensure_ascii=False, indent=2)

    debug_log(f"å›é¥‹è³‡æ–™å·²å„²å­˜è‡³: {file_path}")
    return file_path


def create_feedback_text(feedback_data: dict) -> str:
    """
    å»ºç«‹æ ¼å¼åŒ–çš„å›é¥‹æ–‡å­—

    Args:
        feedback_data: å›é¥‹è³‡æ–™å­—å…¸

    Returns:
        str: æ ¼å¼åŒ–å¾Œçš„å›é¥‹æ–‡å­—
    """
    text_parts = []

    # åŸºæœ¬å›é¥‹å…§å®¹
    if feedback_data.get("interactive_feedback"):
        text_parts.append(f"=== ç”¨æˆ¶å›é¥‹ ===\n{feedback_data['interactive_feedback']}")

    # å‘½ä»¤åŸ·è¡Œæ—¥èªŒ
    if feedback_data.get("command_logs"):
        text_parts.append(f"=== å‘½ä»¤åŸ·è¡Œæ—¥èªŒ ===\n{feedback_data['command_logs']}")

    # åœ–ç‰‡é™„ä»¶æ¦‚è¦
    if feedback_data.get("images"):
        images = feedback_data["images"]
        text_parts.append(f"=== åœ–ç‰‡é™„ä»¶æ¦‚è¦ ===\nç”¨æˆ¶æä¾›äº† {len(images)} å¼µåœ–ç‰‡ï¼š")

        for i, img in enumerate(images, 1):
            size = img.get("size", 0)
            name = img.get("name", "unknown")

            # æ™ºèƒ½å–®ä½é¡¯ç¤º
            if size < 1024:
                size_str = f"{size} B"
            elif size < 1024 * 1024:
                size_kb = size / 1024
                size_str = f"{size_kb:.1f} KB"
            else:
                size_mb = size / (1024 * 1024)
                size_str = f"{size_mb:.1f} MB"

            img_info = f"  {i}. {name} ({size_str})"

            # ç‚ºæé«˜å…¼å®¹æ€§ï¼Œæ·»åŠ  base64 é è¦½ä¿¡æ¯
            if img.get("data"):
                try:
                    if isinstance(img["data"], bytes):
                        img_base64 = base64.b64encode(img["data"]).decode("utf-8")
                    elif isinstance(img["data"], str):
                        img_base64 = img["data"]
                    else:
                        img_base64 = None

                    if img_base64:
                        # åªé¡¯ç¤ºå‰50å€‹å­—ç¬¦çš„é è¦½
                        preview = (
                            img_base64[:50] + "..."
                            if len(img_base64) > 50
                            else img_base64
                        )
                        img_info += f"\n     Base64 é è¦½: {preview}"
                        img_info += f"\n     å®Œæ•´ Base64 é•·åº¦: {len(img_base64)} å­—ç¬¦"

                        # å¦‚æœ AI åŠ©æ‰‹ä¸æ”¯æ´ MCP åœ–ç‰‡ï¼Œå¯ä»¥æä¾›å®Œæ•´ base64
                        debug_log(f"åœ–ç‰‡ {i} Base64 å·²æº–å‚™ï¼Œé•·åº¦: {len(img_base64)}")

                        # æª¢æŸ¥æ˜¯å¦å•Ÿç”¨ Base64 è©³ç´°æ¨¡å¼ï¼ˆå¾ UI è¨­å®šä¸­ç²å–ï¼‰
                        include_full_base64 = feedback_data.get("settings", {}).get(
                            "enable_base64_detail", False
                        )

                        if include_full_base64:
                            # æ ¹æ“šæª”æ¡ˆåæ¨æ–· MIME é¡å‹
                            file_name = img.get("name", "image.png")
                            if file_name.lower().endswith((".jpg", ".jpeg")):
                                mime_type = "image/jpeg"
                            elif file_name.lower().endswith(".gif"):
                                mime_type = "image/gif"
                            elif file_name.lower().endswith(".webp"):
                                mime_type = "image/webp"
                            else:
                                mime_type = "image/png"

                            img_info += f"\n     å®Œæ•´ Base64: data:{mime_type};base64,{img_base64}"

                except Exception as e:
                    debug_log(f"åœ–ç‰‡ {i} Base64 è™•ç†å¤±æ•—: {e}")

            text_parts.append(img_info)

        # æ·»åŠ å…¼å®¹æ€§èªªæ˜
        text_parts.append(
            "\nğŸ’¡ æ³¨æ„ï¼šå¦‚æœ AI åŠ©æ‰‹ç„¡æ³•é¡¯ç¤ºåœ–ç‰‡ï¼Œåœ–ç‰‡æ•¸æ“šå·²åŒ…å«åœ¨ä¸Šè¿° Base64 ä¿¡æ¯ä¸­ã€‚"
        )

    return "\n\n".join(text_parts) if text_parts else "ç”¨æˆ¶æœªæä¾›ä»»ä½•å›é¥‹å…§å®¹ã€‚"


def process_images(images_data: list[dict]) -> list[MCPImage]:
    """
    è™•ç†åœ–ç‰‡è³‡æ–™ï¼Œè½‰æ›ç‚º MCP åœ–ç‰‡å°è±¡

    Args:
        images_data: åœ–ç‰‡è³‡æ–™åˆ—è¡¨

    Returns:
        List[MCPImage]: MCP åœ–ç‰‡å°è±¡åˆ—è¡¨
    """
    mcp_images = []

    for i, img in enumerate(images_data, 1):
        try:
            if not img.get("data"):
                debug_log(f"åœ–ç‰‡ {i} æ²’æœ‰è³‡æ–™ï¼Œè·³é")
                continue

            # æª¢æŸ¥æ•¸æ“šé¡å‹ä¸¦ç›¸æ‡‰è™•ç†
            if isinstance(img["data"], bytes):
                # å¦‚æœæ˜¯åŸå§‹ bytes æ•¸æ“šï¼Œç›´æ¥ä½¿ç”¨
                image_bytes = img["data"]
                debug_log(
                    f"åœ–ç‰‡ {i} ä½¿ç”¨åŸå§‹ bytes æ•¸æ“šï¼Œå¤§å°: {len(image_bytes)} bytes"
                )
            elif isinstance(img["data"], str):
                # å¦‚æœæ˜¯ base64 å­—ç¬¦ä¸²ï¼Œé€²è¡Œè§£ç¢¼
                image_bytes = base64.b64decode(img["data"])
                debug_log(f"åœ–ç‰‡ {i} å¾ base64 è§£ç¢¼ï¼Œå¤§å°: {len(image_bytes)} bytes")
            else:
                debug_log(f"åœ–ç‰‡ {i} æ•¸æ“šé¡å‹ä¸æ”¯æ´: {type(img['data'])}")
                continue

            if len(image_bytes) == 0:
                debug_log(f"åœ–ç‰‡ {i} æ•¸æ“šç‚ºç©ºï¼Œè·³é")
                continue

            # æ ¹æ“šæ–‡ä»¶åæ¨æ–·æ ¼å¼
            file_name = img.get("name", "image.png")
            if file_name.lower().endswith((".jpg", ".jpeg")):
                image_format = "jpeg"
            elif file_name.lower().endswith(".gif"):
                image_format = "gif"
            else:
                image_format = "png"  # é»˜èªä½¿ç”¨ PNG

            # å‰µå»º MCPImage å°è±¡
            mcp_image = MCPImage(data=image_bytes, format=image_format)
            mcp_images.append(mcp_image)

            debug_log(f"åœ–ç‰‡ {i} ({file_name}) è™•ç†æˆåŠŸï¼Œæ ¼å¼: {image_format}")

        except Exception as e:
            # ä½¿ç”¨çµ±ä¸€éŒ¯èª¤è™•ç†ï¼ˆä¸å½±éŸ¿ JSON RPCï¼‰
            error_id = ErrorHandler.log_error_with_context(
                e,
                context={"operation": "åœ–ç‰‡è™•ç†", "image_index": i},
                error_type=ErrorType.FILE_IO,
            )
            debug_log(f"åœ–ç‰‡ {i} è™•ç†å¤±æ•— [éŒ¯èª¤ID: {error_id}]: {e}")

    debug_log(f"å…±è™•ç† {len(mcp_images)} å¼µåœ–ç‰‡")
    return mcp_images


# ===== MCP å·¥å…·å®šç¾© =====
@mcp.tool()
async def interactive_feedback(
    project_directory: Annotated[str, Field(description="å°ˆæ¡ˆç›®éŒ„è·¯å¾‘")] = ".",
    summary: Annotated[
        str, Field(description="AI å·¥ä½œå®Œæˆçš„æ‘˜è¦èªªæ˜")
    ] = "æˆ‘å·²å®Œæˆäº†æ‚¨è«‹æ±‚çš„ä»»å‹™ã€‚",
    timeout: Annotated[int, Field(description="ç­‰å¾…ç”¨æˆ¶å›é¥‹çš„è¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰")] = 600,
) -> list:
    """
    æ”¶é›†ç”¨æˆ¶çš„äº’å‹•å›é¥‹ï¼Œæ”¯æ´æ–‡å­—å’Œåœ–ç‰‡

    æ­¤å·¥å…·ä½¿ç”¨ Web UI ä»‹é¢æ”¶é›†ç”¨æˆ¶å›é¥‹ï¼Œæ”¯æ´æ™ºèƒ½ç’°å¢ƒæª¢æ¸¬ã€‚

    ç”¨æˆ¶å¯ä»¥ï¼š
    1. åŸ·è¡Œå‘½ä»¤ä¾†é©—è­‰çµæœ
    2. æä¾›æ–‡å­—å›é¥‹
    3. ä¸Šå‚³åœ–ç‰‡ä½œç‚ºå›é¥‹
    4. æŸ¥çœ‹ AI çš„å·¥ä½œæ‘˜è¦

    èª¿è©¦æ¨¡å¼ï¼š
    - è¨­ç½®ç’°å¢ƒè®Šæ•¸ MCP_DEBUG=true å¯å•Ÿç”¨è©³ç´°èª¿è©¦è¼¸å‡º
    - ç”Ÿç”¢ç’°å¢ƒå»ºè­°é—œé–‰èª¿è©¦æ¨¡å¼ä»¥é¿å…è¼¸å‡ºå¹²æ“¾

    Args:
        project_directory: å°ˆæ¡ˆç›®éŒ„è·¯å¾‘
        summary: AI å·¥ä½œå®Œæˆçš„æ‘˜è¦èªªæ˜
        timeout: ç­‰å¾…ç”¨æˆ¶å›é¥‹çš„è¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰ï¼Œé è¨­ç‚º 600 ç§’ï¼ˆ10 åˆ†é˜ï¼‰

    Returns:
        List: åŒ…å« TextContent å’Œ MCPImage å°è±¡çš„åˆ—è¡¨
    """
    # ç’°å¢ƒåµæ¸¬
    is_remote = is_remote_environment()
    is_wsl = is_wsl_environment()

    debug_log(f"ç’°å¢ƒåµæ¸¬çµæœ - é ç«¯: {is_remote}, WSL: {is_wsl}")
    debug_log("ä½¿ç”¨ä»‹é¢: Web UI")

    try:
        # ç¢ºä¿å°ˆæ¡ˆç›®éŒ„å­˜åœ¨
        if not os.path.exists(project_directory):
            project_directory = os.getcwd()
        project_directory = os.path.abspath(project_directory)

        # ä½¿ç”¨ Web æ¨¡å¼
        debug_log("å›é¥‹æ¨¡å¼: web")

        result = await launch_web_feedback_ui(project_directory, summary, timeout)

        # è™•ç†å–æ¶ˆæƒ…æ³
        if not result:
            return [TextContent(type="text", text="ç”¨æˆ¶å–æ¶ˆäº†å›é¥‹ã€‚")]

        # å„²å­˜è©³ç´°çµæœ
        save_feedback_to_file(result)

        # å»ºç«‹å›é¥‹é …ç›®åˆ—è¡¨
        feedback_items = []

        # æ·»åŠ æ–‡å­—å›é¥‹
        if (
            result.get("interactive_feedback")
            or result.get("command_logs")
            or result.get("images")
        ):
            feedback_text = create_feedback_text(result)
            feedback_items.append(TextContent(type="text", text=feedback_text))
            debug_log("æ–‡å­—å›é¥‹å·²æ·»åŠ ")

        # æ·»åŠ åœ–ç‰‡å›é¥‹
        if result.get("images"):
            mcp_images = process_images(result["images"])
            # ä¿®å¾© arg-type éŒ¯èª¤ - ç›´æ¥æ“´å±•åˆ—è¡¨
            feedback_items.extend(mcp_images)
            debug_log(f"å·²æ·»åŠ  {len(mcp_images)} å¼µåœ–ç‰‡")

        # ç¢ºä¿è‡³å°‘æœ‰ä¸€å€‹å›é¥‹é …ç›®
        if not feedback_items:
            feedback_items.append(
                TextContent(type="text", text="ç”¨æˆ¶æœªæä¾›ä»»ä½•å›é¥‹å…§å®¹ã€‚")
            )

        debug_log(f"å›é¥‹æ”¶é›†å®Œæˆï¼Œå…± {len(feedback_items)} å€‹é …ç›®")
        return feedback_items

    except Exception as e:
        # ä½¿ç”¨çµ±ä¸€éŒ¯èª¤è™•ç†ï¼Œä½†ä¸å½±éŸ¿ JSON RPC éŸ¿æ‡‰
        error_id = ErrorHandler.log_error_with_context(
            e,
            context={"operation": "å›é¥‹æ”¶é›†", "project_dir": project_directory},
            error_type=ErrorType.SYSTEM,
        )

        # ç”Ÿæˆç”¨æˆ¶å‹å¥½çš„éŒ¯èª¤ä¿¡æ¯
        user_error_msg = ErrorHandler.format_user_error(e, include_technical=False)
        debug_log(f"å›é¥‹æ”¶é›†éŒ¯èª¤ [éŒ¯èª¤ID: {error_id}]: {e!s}")

        return [TextContent(type="text", text=user_error_msg)]


async def launch_web_feedback_ui(project_dir: str, summary: str, timeout: int) -> dict:
    """
    å•Ÿå‹• Web UI æ”¶é›†å›é¥‹ï¼Œæ”¯æ´è‡ªè¨‚è¶…æ™‚æ™‚é–“

    Args:
        project_dir: å°ˆæ¡ˆç›®éŒ„è·¯å¾‘
        summary: AI å·¥ä½œæ‘˜è¦
        timeout: è¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰

    Returns:
        dict: æ”¶é›†åˆ°çš„å›é¥‹è³‡æ–™
    """
    debug_log(f"å•Ÿå‹• Web UI ä»‹é¢ï¼Œè¶…æ™‚æ™‚é–“: {timeout} ç§’")

    try:
        # ä½¿ç”¨æ–°çš„ web æ¨¡çµ„
        from .web import launch_web_feedback_ui as web_launch

        # å‚³é timeout åƒæ•¸çµ¦ Web UI
        return await web_launch(project_dir, summary, timeout)
    except ImportError as e:
        # ä½¿ç”¨çµ±ä¸€éŒ¯èª¤è™•ç†
        error_id = ErrorHandler.log_error_with_context(
            e,
            context={"operation": "Web UI æ¨¡çµ„å°å…¥", "module": "web"},
            error_type=ErrorType.DEPENDENCY,
        )
        user_error_msg = ErrorHandler.format_user_error(
            e, ErrorType.DEPENDENCY, include_technical=False
        )
        debug_log(f"Web UI æ¨¡çµ„å°å…¥å¤±æ•— [éŒ¯èª¤ID: {error_id}]: {e}")

        return {
            "command_logs": "",
            "interactive_feedback": user_error_msg,
            "images": [],
        }


@mcp.tool()
def get_system_info() -> str:
    """
    ç²å–ç³»çµ±ç’°å¢ƒè³‡è¨Š

    Returns:
        str: JSON æ ¼å¼çš„ç³»çµ±è³‡è¨Š
    """
    is_remote = is_remote_environment()
    is_wsl = is_wsl_environment()

    system_info = {
        "å¹³å°": sys.platform,
        "Python ç‰ˆæœ¬": sys.version.split()[0],
        "WSL ç’°å¢ƒ": is_wsl,
        "é ç«¯ç’°å¢ƒ": is_remote,
        "ä»‹é¢é¡å‹": "Web UI",
        "ç’°å¢ƒè®Šæ•¸": {
            "SSH_CONNECTION": os.getenv("SSH_CONNECTION"),
            "SSH_CLIENT": os.getenv("SSH_CLIENT"),
            "DISPLAY": os.getenv("DISPLAY"),
            "VSCODE_INJECTION": os.getenv("VSCODE_INJECTION"),
            "SESSIONNAME": os.getenv("SESSIONNAME"),
            "WSL_DISTRO_NAME": os.getenv("WSL_DISTRO_NAME"),
            "WSL_INTEROP": os.getenv("WSL_INTEROP"),
            "WSLENV": os.getenv("WSLENV"),
        },
    }

    return json.dumps(system_info, ensure_ascii=False, indent=2)


# ===== è·¯å¾„ç®¡ç†å·¥å…· =====

@mcp.tool()
def get_current_path(
    project_path: Annotated[str, Field(description="é¡¹ç›®æ ¹ç›®å½•è·¯å¾„ï¼Œå¿…é¡»æŒ‡å®š")] = "",
) -> str:
    """
    è·å–å½“å‰é¡¹ç›®è·¯å¾„å¹¶æ£€æŸ¥åˆå§‹åŒ–çŠ¶æ€
    
    æ£€æŸ¥æŒ‡å®šé¡¹ç›®æ˜¯å¦å·²ç»åˆå§‹åŒ–ï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ° .EDATA ç›®å½•å’Œé…ç½®æ–‡ä»¶ï¼Œ
    ä¼šæç¤ºéœ€è¦è¿›è¡Œåˆå§‹åŒ–ã€‚
    
    Args:
        project_path: é¡¹ç›®æ ¹ç›®å½•è·¯å¾„ï¼Œå¿…é¡»æŒ‡å®š
    
    Returns:
        str: JSONæ ¼å¼çš„è·¯å¾„ä¿¡æ¯å’Œåˆå§‹åŒ–çŠ¶æ€
    """
    try:
        # éªŒè¯è·¯å¾„å‚æ•°
        if not project_path:
            raise ValueError("project_path å‚æ•°å¿…é¡»æŒ‡å®šï¼Œä¸èƒ½ä¸ºç©º")
        
        # è·å–é¡¹ç›®æ ¹ç›®å½•
        project_root = Path(project_path).resolve()
        
        # æ£€æŸ¥ .EDATA ç›®å½•
        edata_path = project_root / ".EDATA"
        config_file = edata_path / "config.json"
        
        # æ£€æŸ¥åˆå§‹åŒ–çŠ¶æ€
        is_initialized = edata_path.exists() and config_file.exists()
        
        result = {
            "success": True,
            "project_root": str(project_root),
            "edata_path": str(edata_path),
            "config_file": str(config_file),
            "is_initialized": is_initialized,
            "status": "initialized" if is_initialized else "not_initialized",
            "message": "é¡¹ç›®å·²åˆå§‹åŒ–" if is_initialized else "é¡¹ç›®æœªåˆå§‹åŒ–ï¼Œéœ€è¦è°ƒç”¨ init_path è¿›è¡Œåˆå§‹åŒ–",
            "next_action": None if is_initialized else "è¯·è°ƒç”¨ init_path å·¥å…·è¿›è¡Œé¡¹ç›®åˆå§‹åŒ–",
            "paths": {
                "project_root": str(project_root),
                "edata_directory": str(edata_path),
                "config_file": str(config_file),
                "dags_directory": str(edata_path / "dags") if edata_path.exists() else str(edata_path / "dags"),
                "backups_directory": str(edata_path / "backups") if edata_path.exists() else str(edata_path / "backups"),
                "temp_directory": str(edata_path / "temp") if edata_path.exists() else str(edata_path / "temp")
            },
            "environment": {
                "python_executable": sys.executable,
                "script_location": str(Path(__file__).resolve().parent),
                "platform": sys.platform,
                "python_version": sys.version.split()[0]
            }
        }
        
        debug_log(f"è·å–å½“å‰è·¯å¾„æˆåŠŸ - é¡¹ç›®æ ¹ç›®å½•: {project_root}, åˆå§‹åŒ–çŠ¶æ€: {is_initialized}")
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        error_result = {
            "success": False,
            "error": str(e),
            "error_type": "path_detection_error",
            "message": f"è·å–é¡¹ç›®è·¯å¾„å¤±è´¥: {e}"
        }
        debug_log(f"è·å–é¡¹ç›®è·¯å¾„å¤±è´¥: {e}")
        return json.dumps(error_result, ensure_ascii=False, indent=2)


@mcp.tool()
def set_path(
    project_path: Annotated[str, Field(description="é¡¹ç›®æ ¹ç›®å½•è·¯å¾„ï¼Œå¿…é¡»æŒ‡å®š")] = "",
) -> str:
    """
    æ™ºèƒ½è®¾ç½®é¡¹ç›®è·¯å¾„ - è‡ªåŠ¨æ£€æŸ¥å¹¶åˆå§‹åŒ–
    
    æ£€æŸ¥æŒ‡å®šè·¯å¾„æ˜¯å¦å·²ç»æœ‰å®Œæ•´çš„ .EDATA ç›®å½•ç»“æ„å’Œé…ç½®æ–‡ä»¶ï¼š
    - å¦‚æœå·²å­˜åœ¨ä¸”å®Œæ•´ï¼Œç›´æ¥è®¾ç½®ä¸ºå·¥ä½œè·¯å¾„
    - å¦‚æœä¸å­˜åœ¨æˆ–ä¸å®Œæ•´ï¼Œè‡ªåŠ¨è¿›è¡Œåˆå§‹åŒ–
    è¿™æ˜¯ä¸€ä¸ªæ™ºèƒ½åŒ–çš„æ–¹æ³•ï¼Œæ›¿ä»£æ‰‹åŠ¨è°ƒç”¨ init_path
    
    Args:
        project_path: é¡¹ç›®æ ¹ç›®å½•è·¯å¾„ï¼Œå¿…é¡»æŒ‡å®š
        
    Returns:
        str: JSONæ ¼å¼çš„è®¾ç½®ç»“æœå’Œæ“ä½œè¯´æ˜
    """
    try:
        # éªŒè¯è·¯å¾„å‚æ•°
        if not project_path:
            raise ValueError("project_path å‚æ•°å¿…é¡»æŒ‡å®šï¼Œä¸èƒ½ä¸ºç©º")
        
        # ç¡®å®šé¡¹ç›®è·¯å¾„
        base_path = Path(project_path).resolve()
        
        debug_log(f"è®¾ç½®é¡¹ç›®è·¯å¾„: {base_path}")
        
        # æ£€æŸ¥ .EDATA ç›®å½•ç»“æ„æ˜¯å¦å®Œæ•´
        edata_path = base_path / ".EDATA"
        config_file = edata_path / "config.json"
        init_record_file = edata_path / "initialization_record.json"
        
        required_dirs = [
            edata_path / "dags",
            edata_path / "backups", 
            edata_path / "temp",
            edata_path / "configs",
            edata_path / "sessions",
            edata_path / "logs"
        ]
        
        # æ£€æŸ¥æ˜¯å¦å·²å®Œæ•´åˆå§‹åŒ–
        is_fully_initialized = (
            edata_path.exists() and 
            config_file.exists() and
            all(dir_path.exists() for dir_path in required_dirs)
        )
        
        if is_fully_initialized:
            # å·²å®Œæ•´åˆå§‹åŒ–ï¼Œç›´æ¥è®¾ç½®è·¯å¾„
            debug_log("é¡¹ç›®å·²å®Œæ•´åˆå§‹åŒ–ï¼Œç›´æ¥è®¾ç½®è·¯å¾„")
            
            # è¯»å–ç°æœ‰é…ç½®
            with open(config_file, 'r', encoding='utf-8') as f:
                existing_config = json.load(f)
            
            result = {
                "success": True,
                "action": "path_set",
                "message": "é¡¹ç›®è·¯å¾„è®¾ç½®æˆåŠŸï¼Œä½¿ç”¨ç°æœ‰é…ç½®",
                "project_path": str(base_path),
                "edata_path": str(edata_path),
                "config_file": str(config_file),
                "was_already_initialized": True,
                "existing_config": existing_config,
                "verification": {
                    "config_file_exists": config_file.exists(),
                    "all_directories_exist": all(dir_path.exists() for dir_path in required_dirs),
                    "project_name": existing_config.get("project_info", {}).get("name", "unknown")
                }
            }
            
            debug_log(f"è·¯å¾„è®¾ç½®å®Œæˆï¼Œä½¿ç”¨ç°æœ‰é¡¹ç›®: {existing_config.get('project_info', {}).get('name')}")
            
        else:
            # éœ€è¦åˆå§‹åŒ–ï¼Œæ‰§è¡Œå®Œæ•´çš„åˆå§‹åŒ–æµç¨‹
            debug_log("é¡¹ç›®æœªå®Œæ•´åˆå§‹åŒ–ï¼Œå¼€å§‹è‡ªåŠ¨åˆå§‹åŒ–")
            
            # åˆ›å»º .EDATA ç›®å½•ç»“æ„
            directories_to_create = [
                edata_path,
                edata_path / "dags",
                edata_path / "backups",
                edata_path / "temp", 
                edata_path / "configs",
                edata_path / "sessions",
                edata_path / "logs"
            ]
            
            created_dirs = []
            for directory in directories_to_create:
                if not directory.exists():
                    directory.mkdir(parents=True, exist_ok=True)
                    created_dirs.append(str(directory))
                    debug_log(f"åˆ›å»ºç›®å½•: {directory}")
            
            # åˆ›å»ºé…ç½®æ–‡ä»¶
            config_data = {
                "project_info": {
                    "name": base_path.name,
                    "root_path": str(base_path),
                    "edata_path": str(edata_path),
                    "created_at": datetime.datetime.now().isoformat(),
                    "version": "1.0.0",
                    "initialized_by": "set_path_auto_init"
                },
                "dag_storage": {
                    "base_path": str(edata_path),
                    "dags_path": str(edata_path / "dags"),
                    "backups_path": str(edata_path / "backups"),
                    "temp_path": str(edata_path / "temp"),
                    "configs_path": str(edata_path / "configs")
                },
                "memory_layer": {
                    "current_session": None,
                    "last_active_dag": None,
                    "session_count": 0,
                    "layer_status": {
                        "function": "not_started",
                        "logic": "not_started",
                        "code": "not_started", 
                        "order": "not_started"
                    },
                    "execution_history": []
                },
                "settings": {
                    "auto_backup": True,
                    "debug_mode": True,
                    "max_backup_files": 10,
                    "auto_cleanup": True,
                    "log_level": "INFO"
                },
                "statistics": {
                    "total_dags_created": 0,
                    "successful_executions": 0,
                    "failed_executions": 0,
                    "last_execution": None
                }
            }
            
            # å†™å…¥é…ç½®æ–‡ä»¶
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(config_data, f, ensure_ascii=False, indent=2)
            
            # åˆ›å»ºåˆå§‹åŒ–è®°å½•æ–‡ä»¶
            init_record = {
                "initialization_time": datetime.datetime.now().isoformat(),
                "initialization_method": "set_path_auto_init",
                "project_path": str(base_path),
                "edata_path": str(edata_path),
                "created_directories": created_dirs,
                "config_file": str(config_file),
                "initialization_successful": True,
                "system_info": {
                    "platform": sys.platform,
                    "python_version": sys.version.split()[0],
                    "python_executable": sys.executable
                }
            }
            
            with open(init_record_file, 'w', encoding='utf-8') as f:
                json.dump(init_record, f, ensure_ascii=False, indent=2)
            
            result = {
                "success": True,
                "action": "path_set_with_init",
                "message": "é¡¹ç›®è·¯å¾„è®¾ç½®æˆåŠŸï¼Œå·²å®Œæˆè‡ªåŠ¨åˆå§‹åŒ–",
                "project_path": str(base_path),
                "edata_path": str(edata_path),
                "config_file": str(config_file),
                "init_record_file": str(init_record_file),
                "was_already_initialized": False,
                "created_directories": created_dirs,
                "new_config": config_data,
                "initialization_details": init_record
            }
            
            debug_log(f"é¡¹ç›®è·¯å¾„è®¾ç½®å¹¶åˆå§‹åŒ–å®Œæˆ: {base_path}")
        
        # è‡ªåŠ¨æ›´æ–°YAMLç¼“å­˜
        try:
            from .dag_storage.path_cache import get_cache_manager
            cache_manager = get_cache_manager()
            project_name = base_path.name  # ä½¿ç”¨ç›®å½•åä½œä¸ºé¡¹ç›®å
            cache_result = cache_manager.add_project_path(
                project_name=project_name,
                project_path=str(base_path),
                description=f"è‡ªåŠ¨ç¼“å­˜é¡¹ç›®è·¯å¾„ - {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            )
            
            if cache_result["success"]:
                debug_log(f"âœ… é¡¹ç›®è·¯å¾„å·²è‡ªåŠ¨æ·»åŠ åˆ°ç¼“å­˜: {project_name}")
                result["cache_updated"] = True
                result["cache_project_name"] = project_name
            else:
                debug_log(f"âš ï¸ ç¼“å­˜æ›´æ–°å¤±è´¥: {cache_result.get('error')}")
                result["cache_updated"] = False
                result["cache_error"] = cache_result.get('error')
                
        except Exception as e:
            debug_log(f"âš ï¸ ç¼“å­˜æ›´æ–°å‘ç”Ÿå¼‚å¸¸: {e}")
            result["cache_updated"] = False
            result["cache_error"] = str(e)
        
        # æ·»åŠ é€šç”¨çš„ä¸‹ä¸€æ­¥æç¤º
        result["next_steps"] = [
            "å¯ä»¥ä½¿ç”¨ get_current_path æ£€æŸ¥é¡¹ç›®çŠ¶æ€",
            "å¯ä»¥å¼€å§‹ä½¿ç”¨ 4 å±‚ DAG æ„å»ºå·¥å…·",
            "ä½¿ç”¨ build_function_layer_dag å¼€å§‹æ„å»ºåŠŸèƒ½å±‚",
            "é¡¹ç›®è·¯å¾„å·²è‡ªåŠ¨ç¼“å­˜ï¼Œä¸‹æ¬¡å¯ç›´æ¥ä½¿ç”¨å·¥å…·"
        ]
        
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        error_result = {
            "success": False,
            "action": "path_set_failed",
            "error": str(e),
            "error_type": "path_setting_error",
            "message": f"é¡¹ç›®è·¯å¾„è®¾ç½®å¤±è´¥: {e}"
        }
        debug_log(f"é¡¹ç›®è·¯å¾„è®¾ç½®å¤±è´¥: {e}")
        return json.dumps(error_result, ensure_ascii=False, indent=2)


@mcp.tool() 
def init_path(
    project_path: Annotated[str, Field(description="é¡¹ç›®æ ¹ç›®å½•è·¯å¾„ï¼Œå¿…é¡»æŒ‡å®š")] = "",
) -> str:
    """
    åˆå§‹åŒ–é¡¹ç›®è·¯å¾„å’Œé…ç½®
    
    åˆ›å»º .EDATA ç›®å½•ç»“æ„ã€é…ç½®æ–‡ä»¶å’Œåˆå§‹åŒ–é¡¹ç›®è®¾ç½®ã€‚
    è¿™æ˜¯é¡¹ç›®çš„ç¬¬ä¸€æ­¥ï¼Œå¿…é¡»åœ¨ä½¿ç”¨å…¶ä»–DAGå·¥å…·ä¹‹å‰è°ƒç”¨ã€‚
    
    Args:
        project_path: é¡¹ç›®æ ¹ç›®å½•è·¯å¾„ï¼Œå¿…é¡»æŒ‡å®š
        
    Returns:
        str: JSONæ ¼å¼çš„åˆå§‹åŒ–ç»“æœ
    """
    try:
        # éªŒè¯è·¯å¾„å‚æ•°
        if not project_path:
            raise ValueError("project_path å‚æ•°å¿…é¡»æŒ‡å®šï¼Œä¸èƒ½ä¸ºç©º")
        
        # ç¡®å®šé¡¹ç›®è·¯å¾„
        base_path = Path(project_path).resolve()
        
        debug_log(f"å¼€å§‹åˆå§‹åŒ–é¡¹ç›®è·¯å¾„: {base_path}")
        
        # åˆ›å»º .EDATA ç›®å½•ç»“æ„
        edata_path = base_path / ".EDATA"
        
        directories_to_create = [
            edata_path,
            edata_path / "dags",
            edata_path / "backups",
            edata_path / "temp",
            edata_path / "configs",
            edata_path / "sessions",
            edata_path / "logs"
        ]
        
        created_dirs = []
        for directory in directories_to_create:
            if not directory.exists():
                directory.mkdir(parents=True, exist_ok=True)
                created_dirs.append(str(directory))
                debug_log(f"åˆ›å»ºç›®å½•: {directory}")
        
        # åˆ›å»ºé…ç½®æ–‡ä»¶
        config_file = edata_path / "config.json"
        config_data = {
            "project_info": {
                "name": base_path.name,
                "root_path": str(base_path),
                "edata_path": str(edata_path),
                "created_at": datetime.datetime.now().isoformat(),
                "version": "1.0.0"
            },
            "dag_storage": {
                "base_path": str(edata_path),
                "dags_path": str(edata_path / "dags"),
                "backups_path": str(edata_path / "backups"),
                "temp_path": str(edata_path / "temp"),
                "configs_path": str(edata_path / "configs")
            },
            "memory_layer": {
                "current_session": None,
                "last_active_dag": None,
                "session_count": 0,
                "layer_status": {
                    "function": "not_started",
                    "logic": "not_started", 
                    "code": "not_started",
                    "order": "not_started"
                },
                "execution_history": []
            },
            "settings": {
                "auto_backup": True,
                "debug_mode": True,
                "max_backup_files": 10,
                "auto_cleanup": True,
                "log_level": "INFO"
            },
            "statistics": {
                "total_dags_created": 0,
                "successful_executions": 0,
                "failed_executions": 0,
                "last_execution": None
            }
        }
        
        # å†™å…¥é…ç½®æ–‡ä»¶
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config_data, f, ensure_ascii=False, indent=2)
        
        # åˆ›å»ºåˆå§‹åŒ–è®°å½•æ–‡ä»¶
        init_record_file = edata_path / "initialization_record.json"
        init_record = {
            "initialization_time": datetime.datetime.now().isoformat(),
            "project_path": str(base_path),
            "edata_path": str(edata_path),
            "created_directories": created_dirs,
            "config_file": str(config_file),
            "initialization_successful": True,
            "system_info": {
                "platform": sys.platform,
                "python_version": sys.version.split()[0],
                "python_executable": sys.executable
            }
        }
        
        with open(init_record_file, 'w', encoding='utf-8') as f:
            json.dump(init_record, f, ensure_ascii=False, indent=2)
        
        result = {
            "success": True,
            "message": "é¡¹ç›®åˆå§‹åŒ–æˆåŠŸ",
            "project_path": str(base_path),
            "edata_path": str(edata_path),
            "config_file": str(config_file),
            "init_record_file": str(init_record_file),
            "created_directories": created_dirs,
            "storage_config": config_data["dag_storage"],
            "initialization_time": datetime.datetime.now().isoformat(),
            "next_steps": [
                "å¯ä»¥å¼€å§‹ä½¿ç”¨ 4 å±‚ DAG æ„å»ºå·¥å…·",
                "ä½¿ç”¨ build_function_layer_dag å¼€å§‹æ„å»ºåŠŸèƒ½å±‚",
                "ä½¿ç”¨ get_current_path æ£€æŸ¥é¡¹ç›®çŠ¶æ€"
            ]
        }
        
        debug_log(f"é¡¹ç›®åˆå§‹åŒ–å®Œæˆ: {base_path}")
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        error_result = {
            "success": False,
            "error": str(e),
            "error_type": "initialization_error",
            "message": f"é¡¹ç›®åˆå§‹åŒ–å¤±è´¥: {e}"
        }
        debug_log(f"é¡¹ç›®åˆå§‹åŒ–å¤±è´¥: {e}")
        return json.dumps(error_result, ensure_ascii=False, indent=2)


@mcp.tool()
def list_cached_projects() -> str:
    """
    åˆ—å‡ºæ‰€æœ‰ç¼“å­˜çš„é¡¹ç›®è·¯å¾„
    
    ä»YAMLç¼“å­˜ä¸­åˆ—å‡ºæ‰€æœ‰å·²ç¼“å­˜çš„é¡¹ç›®è·¯å¾„ä¿¡æ¯ï¼ŒåŒ…å«é¡¹ç›®çŠ¶æ€éªŒè¯
    
    Returns:
        str: JSONæ ¼å¼çš„é¡¹ç›®åˆ—è¡¨ï¼ŒåŒ…å«é¡¹ç›®åç§°ã€è·¯å¾„ã€æœ‰æ•ˆæ€§ç­‰ä¿¡æ¯
    """
    try:
        from .dag_storage.path_cache import get_cache_manager
        cache_manager = get_cache_manager()
        result = cache_manager.list_projects()
        
        debug_log(f"ç¼“å­˜é¡¹ç›®åˆ—è¡¨æŸ¥è¯¢å®Œæˆï¼Œæ‰¾åˆ° {result.get('total_count', 0)} ä¸ªé¡¹ç›®")
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        error_result = {
            "success": False,
            "error": str(e),
            "error_type": "cache_list_error",
            "message": f"è·å–ç¼“å­˜é¡¹ç›®åˆ—è¡¨å¤±è´¥: {e}"
        }
        debug_log(f"è·å–ç¼“å­˜é¡¹ç›®åˆ—è¡¨å¤±è´¥: {e}")
        return json.dumps(error_result, ensure_ascii=False, indent=2)


@mcp.tool()
def get_cached_project_path(
    project_name: Annotated[str, Field(description="é¡¹ç›®åç§°ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™è¿”å›æœ€è¿‘æ´»è·ƒçš„é¡¹ç›®")] = "",
) -> str:
    """
    ä»ç¼“å­˜ä¸­è·å–é¡¹ç›®è·¯å¾„
    
    Args:
        project_name: é¡¹ç›®åç§°ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™è¿”å›æœ€è¿‘æ´»è·ƒçš„é¡¹ç›®
        
    Returns:
        str: JSONæ ¼å¼çš„é¡¹ç›®è·¯å¾„ä¿¡æ¯
    """
    try:
        from .dag_storage.path_cache import get_cache_manager
        cache_manager = get_cache_manager()
        result = cache_manager.get_project_path(project_name if project_name else None)
        
        if result["success"]:
            debug_log(f"ä»ç¼“å­˜è·å–é¡¹ç›®è·¯å¾„æˆåŠŸ: {result.get('project_name')}")
        else:
            debug_log(f"ä»ç¼“å­˜è·å–é¡¹ç›®è·¯å¾„å¤±è´¥: {result.get('error')}")
        
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        error_result = {
            "success": False,
            "error": str(e),
            "error_type": "cache_get_error",
            "message": f"ä»ç¼“å­˜è·å–é¡¹ç›®è·¯å¾„å¤±è´¥: {e}"
        }
        debug_log(f"ä»ç¼“å­˜è·å–é¡¹ç›®è·¯å¾„å¤±è´¥: {e}")
        return json.dumps(error_result, ensure_ascii=False, indent=2)


@mcp.tool()
def add_project_to_cache(
    project_name: Annotated[str, Field(description="é¡¹ç›®åç§°")] = "",
    project_path: Annotated[str, Field(description="é¡¹ç›®æ ¹ç›®å½•è·¯å¾„")] = "",
    description: Annotated[str, Field(description="é¡¹ç›®æè¿°")] = "",
) -> str:
    """
    æ‰‹åŠ¨æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°ç¼“å­˜
    
    Args:
        project_name: é¡¹ç›®åç§°
        project_path: é¡¹ç›®æ ¹ç›®å½•è·¯å¾„  
        description: é¡¹ç›®æè¿°
        
    Returns:
        str: JSONæ ¼å¼çš„æ·»åŠ ç»“æœ
    """
    try:
        if not project_name:
            raise ValueError("project_name å‚æ•°å¿…é¡»æŒ‡å®š")
        if not project_path:
            raise ValueError("project_path å‚æ•°å¿…é¡»æŒ‡å®š")
        
        from .dag_storage.path_cache import get_cache_manager
        cache_manager = get_cache_manager()
        result = cache_manager.add_project_path(project_name, project_path, description)
        
        if result["success"]:
            debug_log(f"é¡¹ç›®å·²æ‰‹åŠ¨æ·»åŠ åˆ°ç¼“å­˜: {project_name}")
        else:
            debug_log(f"æ·»åŠ é¡¹ç›®åˆ°ç¼“å­˜å¤±è´¥: {result.get('error')}")
        
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        error_result = {
            "success": False,
            "error": str(e),
            "error_type": "cache_add_error",
            "message": f"æ·»åŠ é¡¹ç›®åˆ°ç¼“å­˜å¤±è´¥: {e}"
        }
        debug_log(f"æ·»åŠ é¡¹ç›®åˆ°ç¼“å­˜å¤±è´¥: {e}")
        return json.dumps(error_result, ensure_ascii=False, indent=2)


@mcp.tool()
def remove_project_from_cache(
    project_name: Annotated[str, Field(description="è¦ç§»é™¤çš„é¡¹ç›®åç§°")] = "",
) -> str:
    """
    ä»ç¼“å­˜ä¸­ç§»é™¤é¡¹ç›®
    
    Args:
        project_name: è¦ç§»é™¤çš„é¡¹ç›®åç§°
        
    Returns:
        str: JSONæ ¼å¼çš„ç§»é™¤ç»“æœ
    """
    try:
        if not project_name:
            raise ValueError("project_name å‚æ•°å¿…é¡»æŒ‡å®š")
        
        from .dag_storage.path_cache import get_cache_manager
        cache_manager = get_cache_manager()
        result = cache_manager.remove_project(project_name)
        
        if result["success"]:
            debug_log(f"é¡¹ç›®å·²ä»ç¼“å­˜ç§»é™¤: {project_name}")
        else:
            debug_log(f"ä»ç¼“å­˜ç§»é™¤é¡¹ç›®å¤±è´¥: {result.get('error')}")
        
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        error_result = {
            "success": False,
            "error": str(e),
            "error_type": "cache_remove_error",
            "message": f"ä»ç¼“å­˜ç§»é™¤é¡¹ç›®å¤±è´¥: {e}"
        }
        debug_log(f"ä»ç¼“å­˜ç§»é™¤é¡¹ç›®å¤±è´¥: {e}")
        return json.dumps(error_result, ensure_ascii=False, indent=2)


@mcp.tool()
def get_project_path() -> str:
    """
    è·å–å½“å‰é¡¹ç›®çš„å·¥ä½œè·¯å¾„
    
    è¿”å›å½“å‰å·¥ä½œç›®å½•çš„ç»å¯¹è·¯å¾„ï¼Œç”¨äºDAGæ•°æ®å­˜å‚¨å’Œé…ç½®ç®¡ç†
    
    Returns:
        str: JSONæ ¼å¼çš„é¡¹ç›®è·¯å¾„ä¿¡æ¯
    """
    try:
        import os
        from pathlib import Path
        
        # è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆä½¿ç”¨è„šæœ¬ä½ç½®æ¨æ–­ï¼‰
        script_path = Path(__file__).resolve().parent.parent.parent
        current_working_dir = script_path
        
        # è·å–ç¯å¢ƒä¿¡æ¯
        result = {
            "success": True,
            "paths": {
                "current_working_directory": str(current_working_dir),
                "suggested_dag_data_path": str(current_working_dir / "dag_data"),
                "suggested_config_path": str(current_working_dir / "dag_config.json"),
                "python_executable": sys.executable,
                "script_location": str(Path(__file__).resolve().parent)
            },
            "environment": {
                "python_version": sys.version.split()[0],
                "platform": sys.platform,
                "user_home": str(Path.home()),
                "environment_variables": {
                    "PWD": os.getenv("PWD", "not_set"),
                    "HOME": os.getenv("HOME", "not_set"),
                    "USER": os.getenv("USER", "not_set")
                }
            },
            "recommendations": {
                "dag_storage_base": str(current_working_dir / "dag_data"),
                "config_file": str(current_working_dir / "dag_config.json"),
                "use_absolute_paths": True
            }
        }
        
        debug_log(f"é¡¹ç›®è·¯å¾„å·¥å…·è°ƒç”¨æˆåŠŸï¼Œå½“å‰ç›®å½•: {current_working_dir}")
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        error_result = {
            "success": False,
            "error": str(e),
            "error_type": "path_detection_error"
        }
        debug_log(f"è·å–é¡¹ç›®è·¯å¾„å¤±è´¥: {e}")
        return json.dumps(error_result, ensure_ascii=False, indent=2)


@mcp.tool()
def initialize_project_config(
    project_path: Annotated[str, Field(description="é¡¹ç›®æ ¹ç›®å½•è·¯å¾„")] = "",
) -> str:
    """
    åˆå§‹åŒ–é¡¹ç›®é…ç½®æ–‡ä»¶å’Œç›®å½•ç»“æ„
    
    åœ¨æŒ‡å®šè·¯å¾„ä¸‹åˆ›å»ºDAGæ•°æ®ç›®å½•å’Œé…ç½®æ–‡ä»¶
    
    Args:
        project_path: é¡¹ç›®æ ¹ç›®å½•è·¯å¾„ï¼Œå¦‚æœä¸ºç©ºåˆ™ä½¿ç”¨å½“å‰å·¥ä½œç›®å½•
        
    Returns:
        str: JSONæ ¼å¼çš„åˆå§‹åŒ–ç»“æœ
    """
    try:
        from pathlib import Path
        import os
        
        # ç¡®å®šé¡¹ç›®è·¯å¾„
        if not project_path:
            # ä½¿ç”¨è„šæœ¬æ‰€åœ¨ç›®å½•çš„ä¸Šçº§ç›®å½•ä½œä¸ºé¡¹ç›®æ ¹ç›®å½•
            script_path = Path(__file__).resolve().parent.parent.parent
            base_path = script_path
        else:
            base_path = Path(project_path).resolve()
        
        debug_log(f"åˆå§‹åŒ–é¡¹ç›®é…ç½®ï¼ŒåŸºç¡€è·¯å¾„: {base_path}")
        
        # åˆ›å»ºç›®å½•ç»“æ„
        directories_to_create = [
            base_path / "dag_data",
            base_path / "dag_data" / "dags",
            base_path / "dag_data" / "backups", 
            base_path / "dag_data" / "temp",
            base_path / "dag_data" / "configs"
        ]
        
        created_dirs = []
        for directory in directories_to_create:
            if not directory.exists():
                directory.mkdir(parents=True, exist_ok=True)
                created_dirs.append(str(directory))
                debug_log(f"åˆ›å»ºç›®å½•: {directory}")
        
        # åˆ›å»ºé…ç½®æ–‡ä»¶
        config_file = base_path / "dag_config.json"
        config_data = {
            "project_name": base_path.name,
            "created_at": datetime.datetime.now().isoformat(),
            "dag_storage": {
                "base_path": str(base_path / "dag_data"),
                "dags_path": str(base_path / "dag_data" / "dags"),
                "backups_path": str(base_path / "dag_data" / "backups"),
                "temp_path": str(base_path / "dag_data" / "temp")
            },
            "memory_layer": {
                "current_session": None,
                "last_active_dag": None,
                "layer_status": {
                    "function": "not_started",
                    "logic": "not_started", 
                    "code": "not_started",
                    "order": "not_started"
                }
            },
            "settings": {
                "auto_backup": True,
                "debug_mode": True,
                "max_backup_files": 10
            }
        }
        
        # å†™å…¥é…ç½®æ–‡ä»¶
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config_data, f, ensure_ascii=False, indent=2)
        
        result = {
            "success": True,
            "project_path": str(base_path),
            "config_file": str(config_file),
            "created_directories": created_dirs,
            "storage_config": config_data["dag_storage"],
            "initialization_time": datetime.datetime.now().isoformat()
        }
        
        debug_log(f"é¡¹ç›®é…ç½®åˆå§‹åŒ–å®Œæˆ: {base_path}")
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        error_result = {
            "success": False,
            "error": str(e),
            "error_type": "initialization_error"
        }
        debug_log(f"é¡¹ç›®é…ç½®åˆå§‹åŒ–å¤±è´¥: {e}")
        return json.dumps(error_result, ensure_ascii=False, indent=2)


# ===== å››å±¤ DAG æ§‹å»ºå·¥å…· =====

@mcp.tool()
def build_function_layer_dag(
    project_path: Annotated[str, Field(description="é …ç›®æ ¹ç›®éŒ„è·¯å¾‘ï¼Œå¿…é ˆæŒ‡å®š")] = "",
    project_description: Annotated[str, Field(description="é …ç›®æè¿°å’Œç›®æ¨™")] = "",
    mermaid_dag: Annotated[str, Field(description="åŠŸèƒ½å±¤ Mermaid DAG æè¿°")] = "",
    business_requirements: Annotated[str, Field(description="æ¥­å‹™éœ€æ±‚åˆ—è¡¨")] = "",
) -> str:
    """
    æ§‹å»ºåŠŸèƒ½å±¤ DAG - What Layer (æ¥­å‹™ç›®æ¨™å±¤)
    
    æ­¤å·¥å…·æ¥å— AI æ¨¡å‹è¿”å›çš„åŠŸèƒ½å±¤ Mermaid DAG æè¿°ï¼Œè§£æä¸¦é©—è­‰å…¶åˆæ³•æ€§ï¼Œ
    ç„¶å¾Œè½‰æ›ç‚ºçµ±ä¸€çš„æ•¸æ“šçµæ§‹é€²è¡Œå­˜å„²ã€‚
    
    åŠŸèƒ½å±¤å°ˆæ³¨æ–¼ï¼š
    - æ¥­å‹™åŠŸèƒ½è­˜åˆ¥å’Œåˆ†è§£
    - åŠŸèƒ½æ¨¡å¡Šä¾è³´é—œä¿‚
    - ç”¨æˆ¶éœ€æ±‚åˆ°åŠŸèƒ½çš„æ˜ å°„
    - åŠŸèƒ½å„ªå…ˆç´šå’Œåƒ¹å€¼è©•ä¼°
    
    Args:
        project_path: é …ç›®æ ¹ç›®éŒ„è·¯å¾‘ï¼Œå¿…é ˆæŒ‡å®š
        project_description: é …ç›®çš„æ•´é«”æè¿°å’Œç›®æ¨™
        mermaid_dag: åŠŸèƒ½å±¤çš„ Mermaid DAG æè¿°ï¼ˆç”± AI ç”Ÿæˆï¼‰
        business_requirements: å…·é«”çš„æ¥­å‹™éœ€æ±‚åˆ—è¡¨
        
    Returns:
        str: JSON æ ¼å¼çš„è™•ç†çµæœï¼ŒåŒ…å«æˆåŠŸç‹€æ…‹å’Œ DAG æ•¸æ“š
    """
    try:
        # ç¬¬ä¸€æ­¥ï¼šéªŒè¯å’Œæ£€æŸ¥é¡¹ç›®åˆå§‹åŒ–çŠ¶æ€
        debug_log("é–‹å§‹åŠŸèƒ½å±¤ DAG æ§‹å»º - æª¢æŸ¥é …ç›®åˆå§‹åŒ–ç‹€æ…‹")
        
        # éªŒè¯è·¯å¾„å‚æ•°
        if not project_path:
            raise ValueError("project_path åƒæ•¸å¿…é ˆæŒ‡å®šï¼Œä¸èƒ½ç‚ºç©º")
        
        # ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„é¡¹ç›®è·¯å¾„
        current_path = Path(project_path).resolve()
        edata_path = current_path / ".EDATA"
        config_file = edata_path / "config.json"
        
        debug_log(f"ç•¶å‰é …ç›®è·¯å¾‘: {current_path}")
        debug_log(f"æª¢æŸ¥ .EDATA è·¯å¾‘: {edata_path}")
        debug_log(f"æª¢æŸ¥é…ç½®æ–‡ä»¶: {config_file}")
        
        # æ£€æŸ¥æ˜¯å¦å·²åˆå§‹åŒ–ï¼ˆä½¿ç”¨ .EDATA ç»“æ„ï¼‰
        if not edata_path.exists() or not config_file.exists():
            debug_log("é …ç›®æœªåˆå§‹åŒ–ï¼Œé–‹å§‹è‡ªå‹•åˆå§‹åŒ–...")
            
            # ç›´æ¥æ‰§è¡Œåˆå§‹åŒ–é€»è¾‘ï¼ˆä½¿ç”¨ .EDATA ç»“æ„ï¼‰
            directories_to_create = [
                edata_path,
                edata_path / "dags",
                edata_path / "backups", 
                edata_path / "temp",
                edata_path / "configs",
                edata_path / "sessions",
                edata_path / "logs"
            ]
            
            for directory in directories_to_create:
                if not directory.exists():
                    directory.mkdir(parents=True, exist_ok=True)
                    debug_log(f"åˆ›å»ºç›®å½•: {directory}")
            
            # åˆ›å»ºé…ç½®æ–‡ä»¶ï¼ˆä½¿ç”¨æ–°çš„ .EDATA ç»“æ„ï¼‰
            config_data = {
                "project_info": {
                    "name": current_path.name,
                    "root_path": str(current_path),
                    "edata_path": str(edata_path),
                    "created_at": datetime.datetime.now().isoformat(),
                    "version": "1.0.0",
                    "initialized_by": "build_function_layer_dag_auto_init"
                },
                "dag_storage": {
                    "base_path": str(edata_path),
                    "dags_path": str(edata_path / "dags"),
                    "backups_path": str(edata_path / "backups"),
                    "temp_path": str(edata_path / "temp"),
                    "configs_path": str(edata_path / "configs")
                },
                "memory_layer": {
                    "current_session": None,
                    "last_active_dag": None,
                    "session_count": 0,
                    "layer_status": {
                        "function": "not_started",
                        "logic": "not_started", 
                        "code": "not_started",
                        "order": "not_started"
                    },
                    "execution_history": []
                },
                "settings": {
                    "auto_backup": True,
                    "debug_mode": True,
                    "max_backup_files": 10,
                    "auto_cleanup": True,
                    "log_level": "INFO"
                },
                "statistics": {
                    "total_dags_created": 0,
                    "successful_executions": 0,
                    "failed_executions": 0,
                    "last_execution": None
                }
            }
            
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(config_data, f, ensure_ascii=False, indent=2)
            
            debug_log("é …ç›®åˆå§‹åŒ–å®Œæˆ")
        else:
            debug_log("é …ç›®å·²åˆå§‹åŒ–ï¼Œè¼‰å…¥ç¾æœ‰é…ç½®")
        
        # åŠ è½½é¡¹ç›®é…ç½®
        with open(config_file, 'r', encoding='utf-8') as f:
            project_config = json.load(f)
        
        debug_log(f"é …ç›®é…ç½®è¼‰å…¥å®Œæˆ: {project_config.get('project_info', {}).get('name', 'unknown')}")
        
        # ç¬¬äºŒæ­¥ï¼šå¼€å§‹æ„å»ºåŠŸèƒ½å±‚DAG
        debug_log("é–‹å§‹æ§‹å»ºåŠŸèƒ½å±¤ DAG")
        debug_log(f"é …ç›®æè¿°: {project_description[:100]}...")
        debug_log(f"æ¥æ”¶åˆ° Mermaid DAG é•·åº¦: {len(mermaid_dag)} å­—ç¬¦")
        
        # æ§‹å»ºçµæœæ•¸æ“šçµæ§‹
        result = {
            "success": True,
            "layer_type": "function",
            "layer_name": "åŠŸèƒ½å±¤ (What Layer)",
            "description": "æ¥­å‹™ç›®æ¨™å’ŒåŠŸèƒ½éœ€æ±‚å±¤",
            "input_data": {
                "project_description": project_description,
                "mermaid_dag": mermaid_dag,
                "business_requirements": business_requirements,
                "timestamp": datetime.datetime.now().isoformat()
            },
            "parsed_dag": {
                "nodes": [],
                "edges": [],
                "metadata": {
                    "layer": "function",
                    "focus": ["æ¥­å‹™åŠŸèƒ½è­˜åˆ¥", "åŠŸèƒ½æ¨¡å¡Šä¾è³´", "éœ€æ±‚æ˜ å°„", "å„ªå…ˆç´šè©•ä¼°"],
                    "node_count": 0,
                    "edge_count": 0
                }
            },
            "validation": {
                "is_valid": True,
                "validation_messages": ["åŠŸèƒ½å±¤ DAG æ¥æ”¶æˆåŠŸ"],
                "warnings": [],
                "errors": []
            },
            "storage_info": {
                "format": "unified_dag_model",
                "storage_path": f"function_layer_{hash(mermaid_dag) % 10000}.json",
                "backup_created": True
            }
        }
        
        # ç°¡å–®çš„ Mermaid è§£æï¼ˆç¬¬ä¸€æ­¥å…ˆä¸åšè¤‡é›œé©—è­‰ï¼‰
        if mermaid_dag:
            lines = mermaid_dag.split('\n')
            node_count = sum(1 for line in lines if '-->' in line or '->' in line)
            edge_count = sum(1 for line in lines if '-->' in line or '->' in line)
            
            result["parsed_dag"]["metadata"]["node_count"] = node_count
            result["parsed_dag"]["metadata"]["edge_count"] = edge_count
            
            debug_log(f"åŠŸèƒ½å±¤ DAG è§£æå®Œæˆ - ç¯€é»: {node_count}, é‚Š: {edge_count}")
        
        # ç¬¬ä¸‰æ­¥ï¼šä¿å­˜DAGæ•°æ®
        debug_log("é–‹å§‹ä¿å­˜åŠŸèƒ½å±¤ DAG æ•¸æ“š")
        
        try:
            # ä½¿ç”¨é¡¹ç›®é…ç½®ä¸­çš„å­˜å‚¨è·¯å¾„
            dag_storage_path = Path(project_config["dag_storage"]["dags_path"])
            
            # ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶å
            dag_id = f"function_layer_{hash(mermaid_dag) % 100000}"
            dag_file = dag_storage_path / f"{dag_id}.json"
            
            # ä¿å­˜DAGæ•°æ®
            with open(dag_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            
            # æ›´æ–°å­˜å‚¨ä¿¡æ¯
            result["storage_info"]["file_path"] = str(dag_file)
            result["storage_info"]["dag_id"] = dag_id
            result["storage_info"]["saved_successfully"] = True
            
            # æ›´æ–°é¡¹ç›®é…ç½®ä¸­çš„å±‚çŠ¶æ€
            project_config["memory_layer"]["layer_status"]["function"] = "completed"
            project_config["memory_layer"]["last_active_dag"] = dag_id
            
            # ä¿å­˜æ›´æ–°çš„é…ç½®
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(project_config, f, ensure_ascii=False, indent=2)
            
            debug_log(f"åŠŸèƒ½å±¤ DAG æˆåŠŸä¿å­˜åˆ°: {dag_file}")
            
        except Exception as storage_error:
            debug_log(f"åŠŸèƒ½å±¤ DAG ä¿å­˜å¤±æ•—: {storage_error}")
            result["storage_info"]["error"] = str(storage_error)
            result["storage_info"]["saved_successfully"] = False
        
        # é€šè¿‡WebSocketå‘é€æ›´æ–°
        # try:
        #     from .web.main import send_dag_update
        #     asyncio.run(send_dag_update({
        #         "layer": "function",
        #         "dag": mermaid_dag
        #     }))
        # except Exception as e:
        #     debug_log(f"WebSocketæ›´æ–°å¤±è´¥: {e}")
        debug_log("WebSocketæ›´æ–°æš‚æ—¶ç¦ç”¨")
        
        # è¿”å› JSON å­—ç¬¦ä¸²æ ¼å¼çš„çµæœ
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        error_result = {
            "success": False,
            "layer_type": "function",
            "error": str(e),
            "error_type": "parsing_error",
            "input_data": {
                "project_description": project_description,
                "mermaid_dag_length": len(mermaid_dag) if mermaid_dag else 0
            }
        }
        debug_log(f"åŠŸèƒ½å±¤ DAG æ§‹å»ºå¤±æ•—: {e}")
        return json.dumps(error_result, ensure_ascii=False, indent=2)


@mcp.tool()
def build_logic_layer_dag(
    function_layer_result: Annotated[str, Field(description="åŠŸèƒ½å±¤æ§‹å»ºçµæœ")] = "",
    mermaid_dag: Annotated[str, Field(description="é‚è¼¯å±¤ Mermaid DAG æè¿°")] = "",
    technical_architecture: Annotated[str, Field(description="æŠ€è¡“æ¶æ§‹è¨­è¨ˆ")] = "",
) -> str:
    """
    æ§‹å»ºé‚è¼¯å±¤ DAG - How Layer (æŠ€è¡“æ¶æ§‹å±¤)
    
    æ­¤å·¥å…·æ¥å— AI æ¨¡å‹è¿”å›çš„é‚è¼¯å±¤ Mermaid DAG æè¿°ï¼Œè§£æä¸¦é©—è­‰å…¶åˆæ³•æ€§ï¼Œ
    ç„¶å¾Œè½‰æ›ç‚ºçµ±ä¸€çš„æ•¸æ“šçµæ§‹é€²è¡Œå­˜å„²ã€‚
    
    é‚è¼¯å±¤å°ˆæ³¨æ–¼ï¼š
    - æŠ€è¡“æ¶æ§‹è¨­è¨ˆ
    - ç³»çµ±çµ„ä»¶é—œä¿‚
    - API å’Œæ¥å£å®šç¾©
    - æ•¸æ“šæµå’Œæ§åˆ¶æµ
    
    Args:
        function_layer_result: åŠŸèƒ½å±¤çš„æ§‹å»ºçµæœï¼ˆJSON æ ¼å¼ï¼‰
        mermaid_dag: é‚è¼¯å±¤çš„ Mermaid DAG æè¿°ï¼ˆç”± AI ç”Ÿæˆï¼‰
        technical_architecture: æŠ€è¡“æ¶æ§‹çš„è©³ç´°è¨­è¨ˆ
        
    Returns:
        str: JSON æ ¼å¼çš„è™•ç†çµæœï¼ŒåŒ…å«æˆåŠŸç‹€æ…‹å’Œ DAG æ•¸æ“š
    """
    try:
        debug_log("é–‹å§‹æ§‹å»ºé‚è¼¯å±¤ DAG")
        debug_log(f"åŠŸèƒ½å±¤çµæœé•·åº¦: {len(function_layer_result)} å­—ç¬¦")
        debug_log(f"æ¥æ”¶åˆ° Mermaid DAG é•·åº¦: {len(mermaid_dag)} å­—ç¬¦")
        
        # æ§‹å»ºçµæœæ•¸æ“šçµæ§‹
        result = {
            "success": True,
            "layer_type": "logic",
            "layer_name": "é‚è¼¯å±¤ (How Layer)",
            "description": "æŠ€è¡“æ¶æ§‹å’Œç³»çµ±è¨­è¨ˆå±¤",
            "input_data": {
                "function_layer_result": function_layer_result,
                "mermaid_dag": mermaid_dag,
                "technical_architecture": technical_architecture,
                "timestamp": datetime.datetime.now().isoformat()
            },
            "parsed_dag": {
                "nodes": [],
                "edges": [],
                "metadata": {
                    "layer": "logic",
                    "focus": ["æŠ€è¡“æ¶æ§‹è¨­è¨ˆ", "ç³»çµ±çµ„ä»¶é—œä¿‚", "APIæ¥å£å®šç¾©", "æ•¸æ“šæµæ§åˆ¶"],
                    "node_count": 0,
                    "edge_count": 0,
                    "depends_on": ["function"]
                }
            },
            "validation": {
                "is_valid": True,
                "validation_messages": ["é‚è¼¯å±¤ DAG æ¥æ”¶æˆåŠŸ"],
                "warnings": [],
                "errors": []
            },
            "cross_layer_mapping": {
                "function_to_logic": {},
                "mapping_completeness": 0.0,
                "consistency_check": "pending"
            },
            "storage_info": {
                "format": "unified_dag_model",
                "storage_path": f"logic_layer_{hash(mermaid_dag) % 10000}.json",
                "backup_created": True
            }
        }
        
        # ç°¡å–®çš„ Mermaid è§£æ
        if mermaid_dag:
            lines = mermaid_dag.split('\n')
            node_count = sum(1 for line in lines if '-->' in line or '->' in line)
            edge_count = sum(1 for line in lines if '-->' in line or '->' in line)
            
            result["parsed_dag"]["metadata"]["node_count"] = node_count
            result["parsed_dag"]["metadata"]["edge_count"] = edge_count
            
            debug_log(f"é‚è¼¯å±¤ DAG è§£æå®Œæˆ - ç¯€é»: {node_count}, é‚Š: {edge_count}")
        
        # å®é™…ä¿å­˜åˆ°æ–‡ä»¶ç³»ç»Ÿ
        from .dag_storage.dag_data_storage import get_storage
        storage = get_storage()
        storage_result = storage.save_dag(result)
        
        # æ›´æ–°å­˜å‚¨ä¿¡æ¯
        if storage_result.get("success"):
            result["storage_info"].update(storage_result)
            debug_log(f"é€»è¾‘å±‚æ•°æ®å·²ä¿å­˜åˆ°: {storage_result.get('file_path')}")
        else:
            debug_log(f"é€»è¾‘å±‚æ•°æ®ä¿å­˜å¤±è´¥: {storage_result.get('error')}")
        
        # é€šè¿‡WebSocketå‘é€æ›´æ–°
        try:
            from .web.main import send_dag_update
            asyncio.run(send_dag_update({
                "layer": "logic",
                "dag": mermaid_dag
            }))
        except Exception as e:
            debug_log(f"WebSocketæ›´æ–°å¤±è´¥: {e}")
        
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        error_result = {
            "success": False,
            "layer_type": "logic",
            "error": str(e),
            "error_type": "parsing_error",
            "input_data": {
                "function_layer_result_length": len(function_layer_result) if function_layer_result else 0,
                "mermaid_dag_length": len(mermaid_dag) if mermaid_dag else 0
            }
        }
        debug_log(f"é‚è¼¯å±¤ DAG æ§‹å»ºå¤±æ•—: {e}")
        return json.dumps(error_result, ensure_ascii=False, indent=2)


@mcp.tool()
def build_code_layer_dag(
    logic_layer_result: Annotated[str, Field(description="é‚è¼¯å±¤æ§‹å»ºçµæœ")] = "",
    mermaid_dag: Annotated[str, Field(description="ä»£ç¢¼å±¤ Mermaid DAG æè¿°")] = "",
    implementation_details: Annotated[str, Field(description="å¯¦ç¾ç´°ç¯€å’ŒæŠ€è¡“é¸å‹")] = "",
) -> str:
    """
    æ§‹å»ºä»£ç¢¼å±¤ DAG - Code Layer (å¯¦ç¾æ¶æ§‹å±¤)
    
    æ­¤å·¥å…·æ¥å— AI æ¨¡å‹è¿”å›çš„ä»£ç¢¼å±¤ Mermaid DAG æè¿°ï¼Œè§£æä¸¦é©—è­‰å…¶åˆæ³•æ€§ï¼Œ
    ç„¶å¾Œè½‰æ›ç‚ºçµ±ä¸€çš„æ•¸æ“šçµæ§‹é€²è¡Œå­˜å„²ã€‚
    
    ä»£ç¢¼å±¤å°ˆæ³¨æ–¼ï¼š
    - ä»£ç¢¼æ¨¡å¡Šçµæ§‹
    - æ–‡ä»¶çµ„ç¹”æ¶æ§‹
    - é¡å’Œå‡½æ•¸è¨­è¨ˆ
    - ä¾è³´é—œä¿‚ç®¡ç†
    
    Args:
        logic_layer_result: é‚è¼¯å±¤çš„æ§‹å»ºçµæœï¼ˆJSON æ ¼å¼ï¼‰
        mermaid_dag: ä»£ç¢¼å±¤çš„ Mermaid DAG æè¿°ï¼ˆç”± AI ç”Ÿæˆï¼‰
        implementation_details: å¯¦ç¾ç´°ç¯€å’ŒæŠ€è¡“é¸å‹èªªæ˜
        
    Returns:
        str: JSON æ ¼å¼çš„è™•ç†çµæœï¼ŒåŒ…å«æˆåŠŸç‹€æ…‹å’Œ DAG æ•¸æ“š
    """
    try:
        debug_log("é–‹å§‹æ§‹å»ºä»£ç¢¼å±¤ DAG")
        debug_log(f"é‚è¼¯å±¤çµæœé•·åº¦: {len(logic_layer_result)} å­—ç¬¦")
        debug_log(f"æ¥æ”¶åˆ° Mermaid DAG é•·åº¦: {len(mermaid_dag)} å­—ç¬¦")
        
        # æ§‹å»ºçµæœæ•¸æ“šçµæ§‹
        result = {
            "success": True,
            "layer_type": "code",
            "layer_name": "ä»£ç¢¼å±¤ (Code Layer)",
            "description": "ä»£ç¢¼å¯¦ç¾å’Œæ¨¡å¡Šçµ„ç¹”å±¤",
            "input_data": {
                "logic_layer_result": logic_layer_result,
                "mermaid_dag": mermaid_dag,
                "implementation_details": implementation_details,
                "timestamp": datetime.datetime.now().isoformat()
            },
            "parsed_dag": {
                "nodes": [],
                "edges": [],
                "metadata": {
                    "layer": "code",
                    "focus": ["ä»£ç¢¼æ¨¡å¡Šçµæ§‹", "æ–‡ä»¶çµ„ç¹”æ¶æ§‹", "é¡å‡½æ•¸è¨­è¨ˆ", "ä¾è³´ç®¡ç†"],
                    "node_count": 0,
                    "edge_count": 0,
                    "depends_on": ["function", "logic"]
                }
            },
            "validation": {
                "is_valid": True,
                "validation_messages": ["ä»£ç¢¼å±¤ DAG æ¥æ”¶æˆåŠŸ"],
                "warnings": [],
                "errors": []
            },
            "cross_layer_mapping": {
                "logic_to_code": {},
                "implementation_coverage": 0.0,
                "dependency_analysis": "pending"
            },
            "storage_info": {
                "format": "unified_dag_model",
                "storage_path": f"code_layer_{hash(mermaid_dag) % 10000}.json",
                "backup_created": True
            }
        }
        
        # ç°¡å–®çš„ Mermaid è§£æ
        if mermaid_dag:
            lines = mermaid_dag.split('\n')
            node_count = sum(1 for line in lines if '-->' in line or '->' in line)
            edge_count = sum(1 for line in lines if '-->' in line or '->' in line)
            
            result["parsed_dag"]["metadata"]["node_count"] = node_count
            result["parsed_dag"]["metadata"]["edge_count"] = edge_count
            
            debug_log(f"ä»£ç¢¼å±¤ DAG è§£æå®Œæˆ - ç¯€é»: {node_count}, é‚Š: {edge_count}")

        # å®é™…ä¿å­˜åˆ°æ–‡ä»¶ç³»ç»Ÿ
        from .dag_storage.dag_data_storage import get_storage
        storage = get_storage()
        storage_result = storage.save_dag(result)
        
        # æ›´æ–°å­˜å‚¨ä¿¡æ¯
        if storage_result.get("success"):
            result["storage_info"].update(storage_result)
            debug_log(f"ä»£ç å±‚æ•°æ®å·²ä¿å­˜åˆ°: {storage_result.get('file_path')}")
        else:
            debug_log(f"ä»£ç å±‚æ•°æ®ä¿å­˜å¤±è´¥: {storage_result.get('error')}")
        
        # é€šè¿‡WebSocketå‘é€æ›´æ–°
        try:
            from .web.main import send_dag_update
            asyncio.run(send_dag_update({
                "layer": "code",
                "dag": mermaid_dag
            }))
        except Exception as e:
            debug_log(f"WebSocketæ›´æ–°å¤±è´¥: {e}")
        
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        error_result = {
            "success": False,
            "layer_type": "code",
            "error": str(e),
            "error_type": "parsing_error",
            "input_data": {
                "logic_layer_result_length": len(logic_layer_result) if logic_layer_result else 0,
                "mermaid_dag_length": len(mermaid_dag) if mermaid_dag else 0
            }
        }
        debug_log(f"ä»£ç¢¼å±¤ DAG æ§‹å»ºå¤±æ•—: {e}")
        return json.dumps(error_result, ensure_ascii=False, indent=2)


@mcp.tool()
def build_order_layer_dag(
    code_layer_result: Annotated[str, Field(description="ä»£ç¢¼å±¤æ§‹å»ºçµæœ")] = "",
    mermaid_dag: Annotated[str, Field(description="æ’åºå±¤ Mermaid DAG æè¿°")] = "",
    execution_strategy: Annotated[str, Field(description="åŸ·è¡Œç­–ç•¥å’Œæ™‚åºå®‰æ’")] = "",
) -> str:
    """
    æ§‹å»ºæ’åºå±¤ DAG - When Layer (åŸ·è¡Œé †åºå±¤)
    
    æ­¤å·¥å…·æ¥å— AI æ¨¡å‹è¿”å›çš„æ’åºå±¤ Mermaid DAG æè¿°ï¼Œè§£æä¸¦é©—è­‰å…¶åˆæ³•æ€§ï¼Œ
    ç„¶å¾Œè½‰æ›ç‚ºçµ±ä¸€çš„æ•¸æ“šçµæ§‹é€²è¡Œå­˜å„²ã€‚
    
    æ’åºå±¤å°ˆæ³¨æ–¼ï¼š
    - åŸ·è¡Œé †åºè¦åŠƒ
    - ä»»å‹™ä¾è³´é—œä¿‚
    - è³‡æºåˆ†é…è¨ˆåŠƒ
    - æ™‚é–“ç¯€é»å®‰æ’
    
    Args:
        code_layer_result: ä»£ç¢¼å±¤çš„æ§‹å»ºçµæœï¼ˆJSON æ ¼å¼ï¼‰
        mermaid_dag: æ’åºå±¤çš„ Mermaid DAG æè¿°ï¼ˆç”± AI ç”Ÿæˆï¼‰
        execution_strategy: åŸ·è¡Œç­–ç•¥å’Œæ™‚åºå®‰æ’èªªæ˜
        
    Returns:
        str: JSON æ ¼å¼çš„è™•ç†çµæœï¼ŒåŒ…å«æˆåŠŸç‹€æ…‹å’Œå®Œæ•´çš„å››å±¤ DAG æ•¸æ“š
    """
    try:
        debug_log("é–‹å§‹æ§‹å»ºæ’åºå±¤ DAG")
        debug_log(f"ä»£ç¢¼å±¤çµæœé•·åº¦: {len(code_layer_result)} å­—ç¬¦")
        debug_log(f"æ¥æ”¶åˆ° Mermaid DAG é•·åº¦: {len(mermaid_dag)} å­—ç¬¦")

        # æ§‹å»ºçµæœæ•¸æ“šçµæ§‹
        result = {
            "success": True,
            "layer_type": "order",
            "layer_name": "æ’åºå±¤ (When Layer)",
            "description": "åŸ·è¡Œé †åºå’Œæ™‚åºå®‰æ’å±¤",
            "input_data": {
                "code_layer_result": code_layer_result,
                "mermaid_dag": mermaid_dag,
                "execution_strategy": execution_strategy,
                "timestamp": datetime.datetime.now().isoformat()
            },
            "parsed_dag": {
                "nodes": [],
                "edges": [],
                "metadata": {
                    "layer": "order",
                    "focus": ["åŸ·è¡Œé †åºè¦åŠƒ", "ä»»å‹™ä¾è³´é—œä¿‚", "è³‡æºåˆ†é…è¨ˆåŠƒ", "æ™‚é–“ç¯€é»å®‰æ’"],
                    "node_count": 0,
                    "edge_count": 0,
                    "depends_on": ["function", "logic", "code"]
                }
            },
            "validation": {
                "is_valid": True,
                "validation_messages": ["æ’åºå±¤ DAG æ¥æ”¶æˆåŠŸ", "å››å±¤ DAG æ§‹å»ºå®Œæˆ"],
                "warnings": [],
                "errors": []
            },
            "cross_layer_mapping": {
                "code_to_order": {},
                "execution_feasibility": 0.0,
                "resource_allocation": "pending"
            },
            "four_layer_summary": {
                "all_layers_completed": True,
                "total_layers": 4,
                "layer_sequence": ["function", "logic", "code", "order"],
                "integration_status": "ready_for_validation",
                "next_steps": ["è·¨å±¤ä¸€è‡´æ€§é©—è­‰", "å®Œæ•´æ€§æª¢æŸ¥", "å„ªåŒ–å»ºè­°ç”Ÿæˆ"]
            },
            "storage_info": {
                "format": "unified_dag_model",
                "storage_path": f"order_layer_{hash(mermaid_dag) % 10000}.json",
                "backup_created": True
            }
        }

        # å•ç‹¬æ·»åŠ  complete_dag_path ä»¥é¿å…å¾ªç¯å¼•ç”¨
        result["storage_info"]["complete_dag_path"] = f"complete_four_layer_dag_{hash(mermaid_dag + execution_strategy) % 10000}.json"
        
        # ç°¡å–®çš„ Mermaid è§£æ
        if mermaid_dag:
            lines = mermaid_dag.split('\n')
            node_count = sum(1 for line in lines if '-->' in line or '->' in line)
            edge_count = sum(1 for line in lines if '-->' in line or '->' in line)
            
            result["parsed_dag"]["metadata"]["node_count"] = node_count
            result["parsed_dag"]["metadata"]["edge_count"] = edge_count
            
            debug_log(f"æ’åºå±¤ DAG è§£æå®Œæˆ - ç¯€é»: {node_count}, é‚Š: {edge_count}")
            debug_log("å››å±¤ DAG æ§‹å»ºæµç¨‹å®Œæˆï¼")

        # å®é™…ä¿å­˜åˆ°æ–‡ä»¶ç³»ç»Ÿ
        from .dag_storage.dag_data_storage import get_storage
        storage = get_storage()
        storage_result = storage.save_dag(result)
        
        # æ›´æ–°å­˜å‚¨ä¿¡æ¯
        if storage_result.get("success"):
            result["storage_info"].update(storage_result)
            debug_log(f"æ’åºå±‚æ•°æ®å·²ä¿å­˜åˆ°: {storage_result.get('file_path')}")
        else:
            debug_log(f"æ’åºå±‚æ•°æ®ä¿å­˜å¤±è´¥: {storage_result.get('error')}")
        
        # é€šè¿‡WebSocketå‘é€æ›´æ–°
        try:
            from .web.main import send_dag_update
            asyncio.run(send_dag_update({
                "layer": "order",
                "dag": mermaid_dag
            }))
        except Exception as e:
            debug_log(f"WebSocketæ›´æ–°å¤±è´¥: {e}")
        
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        error_result = {
            "success": False,
            "layer_type": "order",
            "error": str(e),
            "error_type": "parsing_error",
            "input_data": {
                "code_layer_result_length": len(code_layer_result) if code_layer_result else 0,
                "mermaid_dag_length": len(mermaid_dag) if mermaid_dag else 0
            }
        }
        debug_log(f"æ’åºå±¤ DAG æ§‹å»ºå¤±æ•—: {e}")
        return json.dumps(error_result, ensure_ascii=False, indent=2)


# ===== AIæ™ºèƒ½èŠ‚ç‚¹çŠ¶æ€ç®¡ç†å·¥å…· =====

@mcp.tool()
async def ai_identify_current_node(
    dag_data: Annotated[str, Field(description="4å±‚DAGæ•°æ®JSONå­—ç¬¦ä¸²ï¼ŒåŒ…å«å®Œæ•´çš„DAGç»“æ„ä¿¡æ¯")] = "",
    execution_context: Annotated[str, Field(description="æ‰§è¡Œä¸Šä¸‹æ–‡JSONå­—ç¬¦ä¸²ï¼ŒåŒ…å«å½“å‰çŠ¶æ€ã€å®ŒæˆèŠ‚ç‚¹ã€è¿›è¡Œä¸­èŠ‚ç‚¹ç­‰ä¿¡æ¯")] = "",
    additional_info: Annotated[str, Field(description="é¢å¤–ä¿¡æ¯å’Œæç¤ºï¼Œå¦‚ç”¨æˆ·å½“å‰å·¥ä½œã€æœ€è¿‘æ“ä½œç­‰")] = "",
) -> str:
    """
    AIæ™ºèƒ½è¯†åˆ«å½“å‰åº”è¯¥æ‰§è¡Œçš„èŠ‚ç‚¹
    
    æ­¤å·¥å…·ä½¿ç”¨AIåˆ†æ4å±‚DAGç»“æ„å’Œæ‰§è¡ŒçŠ¶æ€ï¼Œæ™ºèƒ½åˆ¤æ–­å½“å‰åº”è¯¥å¤„äºå“ªä¸ªèŠ‚ç‚¹ã€‚
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. åˆ†æ4å±‚DAGç»“æ„å’Œæ‰§è¡ŒçŠ¶æ€
    2. åŸºäºä¸Šä¸‹æ–‡æ™ºèƒ½åˆ¤æ–­å½“å‰ä½ç½®
    3. æä¾›ç½®ä¿¡åº¦å’Œå¤‡é€‰æ–¹æ¡ˆ
    4. ç»™å‡ºè¯¦ç»†çš„åˆ†æreasoning
    
    AIåˆ†æç»´åº¦ï¼š
    - èŠ‚ç‚¹ä¾èµ–å…³ç³»åˆ†æ
    - æ‰§è¡ŒçŠ¶æ€ä¸€è‡´æ€§æ£€æŸ¥
    - ä¸Šä¸‹æ–‡ç†è§£å’Œæ¨ç†
    - é£é™©è¯„ä¼°å’Œå»ºè®®
    
    Args:
        dag_data: åŒ…å«4å±‚DAGå®Œæ•´ç»“æ„çš„JSONæ•°æ®
        execution_context: å½“å‰æ‰§è¡Œä¸Šä¸‹æ–‡ï¼ŒåŒ…æ‹¬å·²å®ŒæˆèŠ‚ç‚¹ã€è¿›è¡Œä¸­èŠ‚ç‚¹ã€é˜»å¡èŠ‚ç‚¹ç­‰
        additional_info: ç”¨æˆ·æä¾›çš„é¢å¤–ä¸Šä¸‹æ–‡ä¿¡æ¯
        
    Returns:
        str: JSONæ ¼å¼çš„èŠ‚ç‚¹ä½ç½®åˆ†æç»“æœï¼ŒåŒ…å«æ¨èèŠ‚ç‚¹ã€ç½®ä¿¡åº¦ã€reasoningç­‰
    """
    try:
        debug_log("å¼€å§‹AIæ™ºèƒ½èŠ‚ç‚¹ä½ç½®è¯†åˆ«")
        
        # å‡†å¤‡AIåˆ†æçš„prompt
        analysis_prompt = f"""
ä½ æ˜¯ä¸€ä¸ª4å±‚DAGæ‰§è¡Œçš„æ™ºèƒ½ä½ç½®è¯†åˆ«ä¸“å®¶ã€‚è¯·åˆ†æå½“å‰é¡¹ç›®æ‰§è¡ŒçŠ¶æ€ï¼Œç¡®å®šåº”è¯¥å¤„äºå“ªä¸ªèŠ‚ç‚¹ã€‚

### 4å±‚DAGæ•°æ®ï¼š
{dag_data}

### æ‰§è¡Œä¸Šä¸‹æ–‡ï¼š
{execution_context}

### é¢å¤–ä¿¡æ¯ï¼š
{additional_info}

è¯·åŸºäºä»¥ä¸‹ç»´åº¦è¿›è¡Œåˆ†æï¼š

1. **ä¾èµ–å…³ç³»åˆ†æ**ï¼šæ£€æŸ¥å“ªäº›èŠ‚ç‚¹çš„å‰ç½®ä¾èµ–å·²æ»¡è¶³
2. **çŠ¶æ€ä¸€è‡´æ€§**ï¼šéªŒè¯å½“å‰çŠ¶æ€ä¸DAGç»“æ„çš„ä¸€è‡´æ€§
3. **ä¸Šä¸‹æ–‡ç†è§£**ï¼šç†è§£é¡¹ç›®ç›®æ ‡å’Œå½“å‰è¿›å±•
4. **ä¼˜å…ˆçº§è¯„ä¼°**ï¼šè€ƒè™‘ä¸šåŠ¡ä»·å€¼å’Œç´§æ€¥ç¨‹åº¦

è¯·è¿”å›ç»“æ„åŒ–çš„JSONåˆ†æç»“æœï¼ŒåŒ…å«ï¼š
{{
  "recommended_node": {{
    "node_id": "æ¨èçš„èŠ‚ç‚¹ID",
    "layer": "æ‰€å±å±‚çº§(function/logic/code/order)",
    "node_name": "èŠ‚ç‚¹åç§°",
    "confidence": 85, // ç½®ä¿¡åº¦(0-100)
    "reasoning": "è¯¦ç»†çš„æ¨ç†è¿‡ç¨‹"
  }},
  "alternatives": [
    {{
      "node_id": "å¤‡é€‰èŠ‚ç‚¹ID",
      "layer": "æ‰€å±å±‚çº§",
      "confidence": 75,
      "reasoning": "é€‰æ‹©ç†ç”±"
    }}
  ],
  "current_analysis": {{
    "ready_nodes": ["å¯æ‰§è¡Œçš„èŠ‚ç‚¹åˆ—è¡¨"],
    "blocked_nodes": ["è¢«é˜»å¡çš„èŠ‚ç‚¹åˆ—è¡¨"],
    "dependencies_status": "ä¾èµ–å…³ç³»çŠ¶æ€åˆ†æ",
    "execution_phase": "å½“å‰æ‰§è¡Œé˜¶æ®µ"
  }},
  "risks_and_suggestions": {{
    "risks": ["è¯†åˆ«çš„é£é™©ç‚¹"],
    "suggestions": ["æ”¹è¿›å»ºè®®"],
    "next_steps": ["åç»­æ­¥éª¤å»ºè®®"]
  }}
}}
"""
        
        # æ¨¡æ‹ŸAIåˆ†æï¼ˆå®é™…å®ç°ä¸­åº”è¯¥è°ƒç”¨LLM APIï¼‰
        # è¿™é‡Œæä¾›ä¸€ä¸ªåŸºç¡€çš„è§„åˆ™å¼•æ“ä½œä¸ºfallback
        import json
        
        try:
            dag_info = json.loads(dag_data) if dag_data else {}
            context_info = json.loads(execution_context) if execution_context else {}
        except:
            dag_info = {}
            context_info = {}
        
        # åŸºç¡€åˆ†æé€»è¾‘
        result = {
            "success": True,
            "analysis_type": "ai_position_identification",
            "timestamp": datetime.datetime.now().isoformat(),
            "recommended_node": {
                "node_id": "auto_detected_node",
                "layer": "function",
                "node_name": "é¡¹ç›®éœ€æ±‚åˆ†æ",
                "confidence": 75,
                "reasoning": "åŸºäºå½“å‰æ‰§è¡ŒçŠ¶æ€å’ŒDAGç»“æ„ï¼Œæ¨èä»åŠŸèƒ½å±‚å¼€å§‹æˆ–ç»§ç»­å½“å‰åŠŸèƒ½å±‚èŠ‚ç‚¹"
            },
            "alternatives": [
                {
                    "node_id": "alternative_node",
                    "layer": "logic", 
                    "confidence": 60,
                    "reasoning": "å¦‚æœåŠŸèƒ½å±‚å·²åŸºæœ¬å®Œæˆï¼Œå¯è€ƒè™‘è¿›å…¥é€»è¾‘å±‚"
                }
            ],
            "current_analysis": {
                "ready_nodes": ["function_node_1", "function_node_2"],
                "blocked_nodes": ["logic_node_1", "code_node_1"],
                "dependencies_status": "åŠŸèƒ½å±‚ä¾èµ–å·²æ»¡è¶³ï¼Œé€»è¾‘å±‚ç­‰å¾…åŠŸèƒ½å±‚å®Œæˆ",
                "execution_phase": "blueprint_construction"
            },
            "risks_and_suggestions": {
                "risks": ["åŠŸèƒ½å±‚å®šä¹‰ä¸å¤Ÿæ¸…æ™°å¯èƒ½å½±å“åç»­å±‚çº§"],
                "suggestions": ["å»ºè®®å…ˆå®Œå–„åŠŸèƒ½å±‚å®šä¹‰", "å»ºç«‹æ¸…æ™°çš„éªŒæ”¶æ ‡å‡†"],
                "next_steps": ["ç»§ç»­å®Œå–„å½“å‰èŠ‚ç‚¹", "å‡†å¤‡ç”¨æˆ·åé¦ˆæ”¶é›†"]
            },
            "ai_analysis": {
                "prompt_used": "ai_position_identification",
                "analysis_depth": "comprehensive",
                "confidence_level": "medium_high",
                "requires_human_validation": True
            }
        }
        
        debug_log(f"AIèŠ‚ç‚¹ä½ç½®è¯†åˆ«å®Œæˆï¼Œæ¨èèŠ‚ç‚¹: {result['recommended_node']['node_id']}")
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        error_result = {
            "success": False,
            "error": str(e),
            "error_type": "ai_analysis_error",
            "timestamp": datetime.datetime.now().isoformat()
        }
        debug_log(f"AIèŠ‚ç‚¹ä½ç½®è¯†åˆ«å¤±è´¥: {e}")
        return json.dumps(error_result, ensure_ascii=False, indent=2)


@mcp.tool()
async def ai_evaluate_node_completion(
    node_id: Annotated[str, Field(description="è¦è¯„ä¼°çš„èŠ‚ç‚¹ID")] = "",
    node_data: Annotated[str, Field(description="èŠ‚ç‚¹æ•°æ®JSONå­—ç¬¦ä¸²ï¼ŒåŒ…å«èŠ‚ç‚¹å®šä¹‰ã€è¾“å‡ºè¦æ±‚ç­‰")] = "",
    completion_evidence: Annotated[str, Field(description="å®Œæˆè¯æ®å’Œè¾“å‡ºï¼Œå¦‚äº¤ä»˜ç‰©ã€æ–‡æ¡£ã€ä»£ç ç­‰")] = "",
    quality_criteria: Annotated[str, Field(description="è´¨é‡æ ‡å‡†å’ŒéªŒæ”¶æ¡ä»¶")] = "",
) -> str:
    """
    AIæ™ºèƒ½è¯„ä¼°èŠ‚ç‚¹å®ŒæˆçŠ¶æ€å’Œè´¨é‡
    
    æ­¤å·¥å…·ä½¿ç”¨AIåˆ†æèŠ‚ç‚¹çš„å®Œæˆæƒ…å†µï¼ŒåŒ…æ‹¬å®Œæˆåº¦ã€è´¨é‡è¯„åˆ†ã€å­˜åœ¨é—®é¢˜ç­‰ã€‚
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. è¯„ä¼°èŠ‚ç‚¹æ˜¯å¦è¾¾åˆ°å®Œæˆæ ‡å‡†
    2. åˆ†æå®Œæˆåº¦å’Œè´¨é‡åˆ†æ•°
    3. è¯†åˆ«å­˜åœ¨çš„é—®é¢˜å’Œé˜»å¡
    4. æä¾›æ”¹è¿›å»ºè®®å’Œä¸‹ä¸€æ­¥è¡ŒåŠ¨
    
    AIè¯„ä¼°ç»´åº¦ï¼š
    - äº¤ä»˜ç‰©å®Œæ•´æ€§æ£€æŸ¥
    - è´¨é‡æ ‡å‡†ç¬¦åˆåº¦è¯„ä¼°
    - éªŒæ”¶æ¡ä»¶æ»¡è¶³æƒ…å†µ
    - æ½œåœ¨é£é™©å’Œæ”¹è¿›ç‚¹
    
    Args:
        node_id: è¦è¯„ä¼°çš„èŠ‚ç‚¹å”¯ä¸€æ ‡è¯†
        node_data: èŠ‚ç‚¹çš„è¯¦ç»†å®šä¹‰å’Œè¦æ±‚
        completion_evidence: å½“å‰çš„å®Œæˆè¯æ®å’Œäº§å‡º
        quality_criteria: è´¨é‡æ ‡å‡†å’ŒéªŒæ”¶æ¡ä»¶
        
    Returns:
        str: JSONæ ¼å¼çš„å®Œæˆåº¦è¯„ä¼°ç»“æœ
    """
    try:
        debug_log(f"å¼€å§‹AIèŠ‚ç‚¹å®Œæˆåº¦è¯„ä¼°ï¼ŒèŠ‚ç‚¹ID: {node_id}")
        
        # å‡†å¤‡AIè¯„ä¼°çš„prompt
        evaluation_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä»»åŠ¡å®Œæˆåº¦è¯„ä¼°ä¸“å®¶ã€‚è¯·è¯„ä¼°æŒ‡å®šèŠ‚ç‚¹çš„å®ŒæˆçŠ¶æ€å’Œè´¨é‡ã€‚

### èŠ‚ç‚¹ä¿¡æ¯ï¼š
- èŠ‚ç‚¹IDï¼š{node_id}
- èŠ‚ç‚¹æ•°æ®ï¼š{node_data}
- è´¨é‡æ ‡å‡†ï¼š{quality_criteria}

### å®Œæˆè¯æ®ï¼š
{completion_evidence}

è¯·ä»ä»¥ä¸‹ç»´åº¦è¿›è¡Œè¯„ä¼°ï¼š

1. **å®Œæ•´æ€§è¯„ä¼°**ï¼šæ£€æŸ¥æ‰€æœ‰è¦æ±‚çš„äº¤ä»˜ç‰©æ˜¯å¦å®Œæˆ
2. **è´¨é‡è¯„ä¼°**ï¼šè¯„ä¼°äº¤ä»˜ç‰©çš„è´¨é‡æ°´å¹³
3. **æ ‡å‡†ç¬¦åˆæ€§**ï¼šéªŒè¯æ˜¯å¦ç¬¦åˆé¢„è®¾çš„éªŒæ”¶æ ‡å‡†
4. **é£é™©è¯†åˆ«**ï¼šè¯†åˆ«æ½œåœ¨é—®é¢˜å’Œé£é™©ç‚¹

è¯·è¿”å›ç»“æ„åŒ–çš„JSONè¯„ä¼°ç»“æœï¼š
{{
  "completion_status": {{
    "is_completed": true/false,
    "completion_percentage": 85, // å®Œæˆåº¦ç™¾åˆ†æ¯”(0-100)
    "quality_score": 78, // è´¨é‡è¯„åˆ†(0-100)
    "meets_standards": true/false
  }},
  "detailed_assessment": {{
    "deliverables_check": {{
      "required": ["éœ€è¦çš„äº¤ä»˜ç‰©åˆ—è¡¨"],
      "completed": ["å·²å®Œæˆçš„äº¤ä»˜ç‰©"],
      "missing": ["ç¼ºå¤±çš„äº¤ä»˜ç‰©"]
    }},
    "quality_analysis": {{
      "strengths": ["ä¼˜ç‚¹å’Œäº®ç‚¹"],
      "weaknesses": ["ä¸è¶³å’Œé—®é¢˜"],
      "improvement_areas": ["æ”¹è¿›æ–¹å‘"]
    }}
  }},
  "blockers_and_issues": [
    {{
      "type": "é—®é¢˜ç±»å‹",
      "description": "é—®é¢˜æè¿°",
      "severity": "high/medium/low",
      "suggested_solution": "å»ºè®®è§£å†³æ–¹æ¡ˆ"
    }}
  ],
  "recommendations": {{
    "immediate_actions": ["ç«‹å³è¡ŒåŠ¨å»ºè®®"],
    "long_term_improvements": ["é•¿æœŸæ”¹è¿›å»ºè®®"],
    "next_steps": ["ä¸‹ä¸€æ­¥è¡ŒåŠ¨è®¡åˆ’"]
  }}
}}
"""
        
        # æ¨¡æ‹ŸAIè¯„ä¼°é€»è¾‘
        import json
        
        try:
            node_info = json.loads(node_data) if node_data else {}
        except:
            node_info = {}
        
        # åŸºç¡€è¯„ä¼°é€»è¾‘
        result = {
            "success": True,
            "evaluation_type": "ai_completion_assessment",
            "node_id": node_id,
            "timestamp": datetime.datetime.now().isoformat(),
            "completion_status": {
                "is_completed": True if completion_evidence and len(completion_evidence) > 100 else False,
                "completion_percentage": min(90, max(10, len(completion_evidence) // 10)) if completion_evidence else 0,
                "quality_score": 75,  # åŸºäºè¯æ®è´¨é‡çš„ç®€å•è¯„åˆ†
                "meets_standards": True if completion_evidence and quality_criteria else False
            },
            "detailed_assessment": {
                "deliverables_check": {
                    "required": ["èŠ‚ç‚¹è¾“å‡º", "æ–‡æ¡£è¯´æ˜", "è´¨é‡æ£€æŸ¥"],
                    "completed": ["èŠ‚ç‚¹è¾“å‡º"] if completion_evidence else [],
                    "missing": ["æ–‡æ¡£è¯´æ˜", "è´¨é‡æ£€æŸ¥"] if not completion_evidence else []
                },
                "quality_analysis": {
                    "strengths": ["æä¾›äº†å®Œæˆè¯æ®"] if completion_evidence else [],
                    "weaknesses": ["éœ€è¦æ›´è¯¦ç»†çš„æ–‡æ¡£"] if not quality_criteria else [],
                    "improvement_areas": ["æ ‡å‡†åŒ–æ–‡æ¡£", "è´¨é‡æ£€æŸ¥æµç¨‹"]
                }
            },
            "blockers_and_issues": [
                {
                    "type": "documentation",
                    "description": "ç¼ºå°‘å®Œæ•´çš„æ–‡æ¡£è¯´æ˜",
                    "severity": "medium",
                    "suggested_solution": "è¡¥å……è¯¦ç»†çš„èŠ‚ç‚¹è¾“å‡ºæ–‡æ¡£"
                }
            ] if not completion_evidence else [],
            "recommendations": {
                "immediate_actions": ["å®Œæˆç¼ºå¤±çš„äº¤ä»˜ç‰©", "è¿›è¡Œè´¨é‡æ£€æŸ¥"],
                "long_term_improvements": ["å»ºç«‹æ ‡å‡†åŒ–æµç¨‹", "æ”¹è¿›è´¨é‡æ ‡å‡†"],
                "next_steps": ["æ”¶é›†ç”¨æˆ·åé¦ˆ", "å‡†å¤‡ä¸‹ä¸€èŠ‚ç‚¹"]
            }
        }
        
        debug_log(f"AIèŠ‚ç‚¹å®Œæˆåº¦è¯„ä¼°å®Œæˆï¼Œå®Œæˆç‡: {result['completion_status']['completion_percentage']}%")
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        error_result = {
            "success": False,
            "error": str(e),
            "error_type": "ai_evaluation_error",
            "node_id": node_id,
            "timestamp": datetime.datetime.now().isoformat()
        }
        debug_log(f"AIèŠ‚ç‚¹å®Œæˆåº¦è¯„ä¼°å¤±è´¥: {e}")
        return json.dumps(error_result, ensure_ascii=False, indent=2)


@mcp.tool()
async def ai_recommend_next_node(
    current_node: Annotated[str, Field(description="å½“å‰èŠ‚ç‚¹ä¿¡æ¯JSONå­—ç¬¦ä¸²")] = "",
    dag_data: Annotated[str, Field(description="4å±‚DAGæ•°æ®JSONå­—ç¬¦ä¸²")] = "",
    resource_state: Annotated[str, Field(description="èµ„æºçŠ¶æ€JSONå­—ç¬¦ä¸²ï¼ŒåŒ…å«å¯ç”¨èµ„æºã€çº¦æŸæ¡ä»¶ç­‰")] = "",
    constraints: Annotated[str, Field(description="çº¦æŸæ¡ä»¶ï¼Œå¦‚æ—¶é—´é™åˆ¶ã€ä¼˜å…ˆçº§è¦æ±‚ç­‰")] = "",
) -> str:
    """
    AIæ™ºèƒ½æ¨èä¸‹ä¸€ä¸ªæ‰§è¡ŒèŠ‚ç‚¹
    
    æ­¤å·¥å…·ä½¿ç”¨AIåˆ†æå½“å‰çŠ¶æ€å’ŒDAGç»“æ„ï¼Œæ™ºèƒ½æ¨èæœ€ä¼˜çš„ä¸‹ä¸€ä¸ªæ‰§è¡ŒèŠ‚ç‚¹ã€‚
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. åˆ†æå¯æ‰§è¡ŒèŠ‚ç‚¹å’Œä¾èµ–å…³ç³»
    2. è€ƒè™‘èµ„æºçº¦æŸå’Œä¼˜å…ˆçº§
    3. è¯†åˆ«å¹¶è¡Œæ‰§è¡Œæœºä¼š
    4. æä¾›æ‰§è¡Œè·¯å¾„å»ºè®®
    
    AIæ¨èç»´åº¦ï¼š
    - ä¾èµ–å…³ç³»æ‹“æ‰‘åˆ†æ
    - èµ„æºå¯ç”¨æ€§è¯„ä¼°
    - ä¼˜å…ˆçº§å’Œé£é™©æƒè¡¡
    - å¹¶è¡Œæ‰§è¡Œä¼˜åŒ–
    
    Args:
        current_node: å½“å‰å®Œæˆæˆ–è¿›è¡Œä¸­çš„èŠ‚ç‚¹ä¿¡æ¯
        dag_data: å®Œæ•´çš„4å±‚DAGç»“æ„æ•°æ®
        resource_state: å½“å‰èµ„æºçŠ¶æ€å’Œçº¦æŸ
        constraints: æ‰§è¡Œçº¦æŸæ¡ä»¶
        
    Returns:
        str: JSONæ ¼å¼çš„ä¸‹ä¸€èŠ‚ç‚¹æ¨èç»“æœ
    """
    try:
        debug_log("å¼€å§‹AIä¸‹ä¸€èŠ‚ç‚¹æ¨èåˆ†æ")
        
        # å‡†å¤‡AIæ¨èçš„prompt
        recommendation_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æ‰§è¡Œè·¯å¾„è§„åˆ’ä¸“å®¶ã€‚åŸºäºå½“å‰çŠ¶æ€æ¨èä¸‹ä¸€ä¸ªæœ€ä¼˜æ‰§è¡ŒèŠ‚ç‚¹ã€‚

### å½“å‰èŠ‚ç‚¹ä¿¡æ¯ï¼š
{current_node}

### 4å±‚DAGç»“æ„ï¼š
{dag_data}

### èµ„æºçŠ¶æ€ï¼š
{resource_state}

### çº¦æŸæ¡ä»¶ï¼š
{constraints}

è¯·ä»ä»¥ä¸‹ç»´åº¦è¿›è¡Œåˆ†æï¼š

1. **ä¾èµ–åˆ†æ**ï¼šè¯†åˆ«æ‰€æœ‰å‰ç½®ä¾èµ–å·²æ»¡è¶³çš„èŠ‚ç‚¹
2. **ä¼˜å…ˆçº§è¯„ä¼°**ï¼šè€ƒè™‘ä¸šåŠ¡ä»·å€¼ã€ç´§æ€¥ç¨‹åº¦ã€é£é™©ç­‰å› ç´ 
3. **èµ„æºåŒ¹é…**ï¼šè¯„ä¼°èµ„æºéœ€æ±‚ä¸å¯ç”¨æ€§çš„åŒ¹é…åº¦
4. **å¹¶è¡Œæœºä¼š**ï¼šè¯†åˆ«å¯ä»¥å¹¶è¡Œæ‰§è¡Œçš„èŠ‚ç‚¹ç»„åˆ

è¯·è¿”å›ç»“æ„åŒ–çš„JSONæ¨èç»“æœï¼š
{{
  "primary_recommendation": {{
    "node_id": "æ¨èçš„ä¸»è¦èŠ‚ç‚¹ID",
    "layer": "æ‰€å±å±‚çº§",
    "node_name": "èŠ‚ç‚¹åç§°",
    "confidence": 90, // æ¨èç½®ä¿¡åº¦(0-100)
    "reasoning": "è¯¦ç»†æ¨èç†ç”±",
    "expected_duration": "é¢„ä¼°æ‰§è¡Œæ—¶é—´",
    "resource_requirements": ["æ‰€éœ€èµ„æºåˆ—è¡¨"]
  }},
  "parallel_opportunities": [
    {{
      "node_id": "å¯å¹¶è¡Œæ‰§è¡Œçš„èŠ‚ç‚¹ID",
      "layer": "æ‰€å±å±‚çº§",
      "parallel_confidence": 80,
      "resource_overlap": "èµ„æºå†²çªè¯„ä¼°"
    }}
  ],
  "execution_path": {{
    "immediate_next": ["ç«‹å³å¯æ‰§è¡Œçš„èŠ‚ç‚¹"],
    "short_term": ["çŸ­æœŸå†…å¯æ‰§è¡Œçš„èŠ‚ç‚¹"],
    "long_term": ["é•¿æœŸè§„åˆ’èŠ‚ç‚¹"],
    "critical_path": ["å…³é”®è·¯å¾„èŠ‚ç‚¹"]
  }},
  "risk_assessment": {{
    "identified_risks": ["è¯†åˆ«çš„é£é™©ç‚¹"],
    "mitigation_strategies": ["é£é™©ç¼“è§£ç­–ç•¥"],
    "contingency_plans": ["åº”æ€¥è®¡åˆ’"]
  }}
}}
"""
        
        # æ¨¡æ‹ŸAIæ¨èé€»è¾‘
        import json
        
        try:
            current_info = json.loads(current_node) if current_node else {}
            dag_info = json.loads(dag_data) if dag_data else {}
        except:
            current_info = {}
            dag_info = {}
        
        # åŸºç¡€æ¨èé€»è¾‘
        result = {
            "success": True,
            "recommendation_type": "ai_next_node_analysis",
            "timestamp": datetime.datetime.now().isoformat(),
            "primary_recommendation": {
                "node_id": "next_logical_node",
                "layer": "logic" if current_info.get("layer") == "function" else "function",
                "node_name": "ç³»ç»Ÿæ¶æ„è®¾è®¡" if current_info.get("layer") == "function" else "éœ€æ±‚ç»†åŒ–",
                "confidence": 85,
                "reasoning": "åŸºäºå½“å‰èŠ‚ç‚¹å®Œæˆæƒ…å†µå’Œä¾èµ–å…³ç³»åˆ†æï¼Œæ¨èç»§ç»­åŒå±‚æ·±åŒ–æˆ–è¿›å…¥ä¸‹ä¸€å±‚",
                "expected_duration": "2-4å°æ—¶",
                "resource_requirements": ["åˆ†æèƒ½åŠ›", "è®¾è®¡å·¥å…·", "æ–‡æ¡£å·¥å…·"]
            },
            "parallel_opportunities": [
                {
                    "node_id": "parallel_analysis_node",
                    "layer": "function",
                    "parallel_confidence": 70,
                    "resource_overlap": "ä½å†²çªï¼Œå¯å¹¶è¡Œæ‰§è¡Œ"
                }
            ],
            "execution_path": {
                "immediate_next": ["next_logical_node"],
                "short_term": ["logic_node_1", "logic_node_2"],
                "long_term": ["code_node_1", "order_node_1"],
                "critical_path": ["next_logical_node", "logic_node_1", "code_node_1"]
            },
            "risk_assessment": {
                "identified_risks": ["å‰ç½®æ¡ä»¶ä¸å¤Ÿå®Œæ•´", "èµ„æºåˆ†é…å†²çª"],
                "mitigation_strategies": ["åŠ å¼ºå‰ç½®éªŒè¯", "åŠ¨æ€èµ„æºè°ƒé…"],
                "contingency_plans": ["å¤‡é€‰èŠ‚ç‚¹å‡†å¤‡", "å›æ»šæœºåˆ¶"]
            },
            "optimization_suggestions": {
                "efficiency_tips": ["åˆå¹¶ç›¸ä¼¼ä»»åŠ¡", "ä¼˜åŒ–ä¾èµ–é¡ºåº"],
                "quality_improvements": ["å¢åŠ æ£€æŸ¥ç‚¹", "å¼ºåŒ–éªŒè¯"],
                "user_experience": ["æä¾›è¿›åº¦åé¦ˆ", "å¢åŠ äº¤äº’ç¡®è®¤"]
            }
        }
        
        debug_log(f"AIä¸‹ä¸€èŠ‚ç‚¹æ¨èå®Œæˆï¼Œä¸»è¦æ¨è: {result['primary_recommendation']['node_id']}")
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        error_result = {
            "success": False,
            "error": str(e),
            "error_type": "ai_recommendation_error",
            "timestamp": datetime.datetime.now().isoformat()
        }
        debug_log(f"AIä¸‹ä¸€èŠ‚ç‚¹æ¨èå¤±è´¥: {e}")
        return json.dumps(error_result, ensure_ascii=False, indent=2)


@mcp.tool()
async def ai_decide_state_update(
    node_id: Annotated[str, Field(description="èŠ‚ç‚¹ID")] = "",
    completion_result: Annotated[str, Field(description="å®Œæˆè¯„ä¼°ç»“æœJSONï¼Œæ¥è‡ªai_evaluate_node_completion")] = "",
    impact_scope: Annotated[str, Field(description="å½±å“èŒƒå›´åˆ†æï¼ŒåŒ…å«ä¸‹æ¸¸èŠ‚ç‚¹ã€ä¾èµ–å…³ç³»ç­‰")] = "",
    update_options: Annotated[str, Field(description="å¯é€‰æ›´æ–°æ–¹æ¡ˆï¼Œå¦‚çŠ¶æ€è½¬æ¢é€‰é¡¹")] = "",
) -> str:
    """
    AIæ™ºèƒ½å†³ç­–èŠ‚ç‚¹çŠ¶æ€æ›´æ–°æ–¹æ¡ˆ
    
    æ­¤å·¥å…·ä½¿ç”¨AIåˆ†æèŠ‚ç‚¹å®Œæˆæƒ…å†µå’Œå½±å“èŒƒå›´ï¼Œæ™ºèƒ½å†³å®šæœ€ä¼˜çš„çŠ¶æ€æ›´æ–°ç­–ç•¥ã€‚
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. å†³å®šæœ€ä¼˜çš„çŠ¶æ€è½¬æ¢
    2. åˆ†æçº§è”å½±å“å’Œæ›´æ–°
    3. åˆ¶å®šé€šçŸ¥å’Œå›æ»šç­–ç•¥
    4. è¯„ä¼°æ›´æ–°é£é™©
    
    AIå†³ç­–ç»´åº¦ï¼š
    - çŠ¶æ€è½¬æ¢åˆç†æ€§
    - çº§è”å½±å“è¯„ä¼°
    - å›æ»šé£é™©æ§åˆ¶
    - é€šçŸ¥ä¼˜å…ˆçº§
    
    Args:
        node_id: è¦æ›´æ–°çŠ¶æ€çš„èŠ‚ç‚¹ID
        completion_result: æ¥è‡ªå®Œæˆåº¦è¯„ä¼°çš„ç»“æœ
        impact_scope: çŠ¶æ€æ›´æ–°çš„å½±å“èŒƒå›´åˆ†æ
        update_options: å¯é€‰çš„æ›´æ–°æ–¹æ¡ˆ
        
    Returns:
        str: JSONæ ¼å¼çš„çŠ¶æ€æ›´æ–°å†³ç­–ç»“æœ
    """
    try:
        debug_log(f"å¼€å§‹AIçŠ¶æ€æ›´æ–°å†³ç­–ï¼ŒèŠ‚ç‚¹ID: {node_id}")
        
        # å‡†å¤‡AIå†³ç­–çš„prompt
        decision_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªçŠ¶æ€æ›´æ–°å†³ç­–ä¸“å®¶ã€‚è¯·å†³å®šå¦‚ä½•æ›´æ–°èŠ‚ç‚¹çŠ¶æ€åŠå…¶å½±å“ã€‚

### èŠ‚ç‚¹ä¿¡æ¯ï¼š
- èŠ‚ç‚¹IDï¼š{node_id}
- å®Œæˆè¯„ä¼°ç»“æœï¼š{completion_result}
- å½±å“èŒƒå›´ï¼š{impact_scope}
- æ›´æ–°é€‰é¡¹ï¼š{update_options}

è¯·ä»ä»¥ä¸‹ç»´åº¦è¿›è¡Œå†³ç­–ï¼š

1. **çŠ¶æ€è½¬æ¢è¯„ä¼°**ï¼šåˆ†æå½“å‰çŠ¶æ€åˆ°ç›®æ ‡çŠ¶æ€çš„åˆç†æ€§
2. **å½±å“åˆ†æ**ï¼šè¯„ä¼°çŠ¶æ€æ›´æ–°å¯¹å…¶ä»–èŠ‚ç‚¹å’Œæ•´ä½“é¡¹ç›®çš„å½±å“
3. **é£é™©æ§åˆ¶**ï¼šè¯†åˆ«æ½œåœ¨é£é™©å¹¶åˆ¶å®šç¼“è§£ç­–ç•¥
4. **æ‰§è¡Œç­–ç•¥**ï¼šåˆ¶å®šå…·ä½“çš„æ›´æ–°æ‰§è¡Œæ–¹æ¡ˆ

è¯·è¿”å›ç»“æ„åŒ–çš„JSONå†³ç­–ç»“æœï¼š
{{
  "update_decision": {{
    "new_state": "ç›®æ ‡çŠ¶æ€",
    "state_transition": "çŠ¶æ€è½¬æ¢ç±»å‹",
    "confidence": 88, // å†³ç­–ç½®ä¿¡åº¦(0-100)
    "reasoning": "è¯¦ç»†å†³ç­–ç†ç”±",
    "execution_priority": "high/medium/low"
  }},
  "cascade_updates": [
    {{
      "affected_node_id": "å—å½±å“çš„èŠ‚ç‚¹ID",
      "update_type": "çŠ¶æ€æ›´æ–°ç±»å‹",
      "update_reason": "æ›´æ–°åŸå› ",
      "execution_order": 1
    }}
  ],
  "notification_strategy": {{
    "immediate_notifications": ["éœ€è¦ç«‹å³é€šçŸ¥çš„å¯¹è±¡"],
    "scheduled_notifications": ["è®¡åˆ’é€šçŸ¥çš„å¯¹è±¡"],
    "notification_content": "é€šçŸ¥å†…å®¹æ¨¡æ¿"
  }},
  "rollback_plan": {{
    "rollback_triggers": ["å›æ»šè§¦å‘æ¡ä»¶"],
    "rollback_steps": ["å›æ»šæ­¥éª¤"],
    "rollback_impact": "å›æ»šå½±å“è¯„ä¼°"
  }},
  "risk_mitigation": {{
    "identified_risks": ["è¯†åˆ«çš„é£é™©"],
    "mitigation_actions": ["ç¼“è§£æªæ–½"],
    "monitoring_points": ["ç›‘æ§è¦ç‚¹"]
  }}
}}
"""
        
        # æ¨¡æ‹ŸAIå†³ç­–é€»è¾‘
        import json
        
        try:
            completion_info = json.loads(completion_result) if completion_result else {}
        except:
            completion_info = {}
        
        # åŸºäºå®Œæˆåº¦è¯„ä¼°ç»“æœå†³å®šçŠ¶æ€æ›´æ–°
        is_completed = completion_info.get("completion_status", {}).get("is_completed", False)
        completion_percentage = completion_info.get("completion_status", {}).get("completion_percentage", 0)
        
        # åŸºç¡€å†³ç­–é€»è¾‘
        if completion_percentage >= 90:
            new_state = "completed"
            execution_priority = "high"
        elif completion_percentage >= 70:
            new_state = "near_completion"
            execution_priority = "medium"
        elif completion_percentage >= 30:
            new_state = "in_progress"
            execution_priority = "low"
        else:
            new_state = "needs_attention"
            execution_priority = "high"
        
        result = {
            "success": True,
            "decision_type": "ai_state_update_decision",
            "node_id": node_id,
            "timestamp": datetime.datetime.now().isoformat(),
            "update_decision": {
                "new_state": new_state,
                "state_transition": f"auto_transition_to_{new_state}",
                "confidence": 82,
                "reasoning": f"åŸºäºå®Œæˆåº¦{completion_percentage}%å’Œè´¨é‡è¯„ä¼°ï¼Œå»ºè®®æ›´æ–°ä¸º{new_state}çŠ¶æ€",
                "execution_priority": execution_priority
            },
            "cascade_updates": [
                {
                    "affected_node_id": "downstream_node_1",
                    "update_type": "dependency_ready" if new_state == "completed" else "dependency_pending",
                    "update_reason": f"ä¸Šæ¸¸èŠ‚ç‚¹{node_id}çŠ¶æ€å˜æ›´ä¸º{new_state}",
                    "execution_order": 1
                }
            ] if new_state == "completed" else [],
            "notification_strategy": {
                "immediate_notifications": ["é¡¹ç›®è´Ÿè´£äºº", "ä¸‹æ¸¸èŠ‚ç‚¹è´Ÿè´£äºº"] if execution_priority == "high" else [],
                "scheduled_notifications": ["å›¢é˜Ÿæˆå‘˜", "åˆ©ç›Šç›¸å…³è€…"],
                "notification_content": f"èŠ‚ç‚¹{node_id}çŠ¶æ€å·²æ›´æ–°ä¸º{new_state}ï¼Œå®Œæˆåº¦{completion_percentage}%"
            },
            "rollback_plan": {
                "rollback_triggers": ["è´¨é‡æ£€æŸ¥å¤±è´¥", "ç”¨æˆ·æ‹’ç»éªŒæ”¶", "ä¸‹æ¸¸èŠ‚ç‚¹æ— æ³•å¼€å§‹"],
                "rollback_steps": ["æ¢å¤å‰ä¸€çŠ¶æ€", "é‡æ–°è¯„ä¼°å®Œæˆæ¡ä»¶", "è°ƒæ•´è´¨é‡æ ‡å‡†"],
                "rollback_impact": "ä½å½±å“ï¼Œä¸»è¦å½±å“å½“å‰èŠ‚ç‚¹å’Œç›´æ¥ä¸‹æ¸¸"
            },
            "risk_mitigation": {
                "identified_risks": ["çŠ¶æ€æ›´æ–°è¿‡äºæ¿€è¿›", "ä¸‹æ¸¸å‡†å¤‡ä¸è¶³"],
                "mitigation_actions": ["å¢åŠ éªŒè¯æ£€æŸ¥ç‚¹", "æå‰æ²Ÿé€šä¸‹æ¸¸å‡†å¤‡"],
                "monitoring_points": ["ä¸‹æ¸¸èŠ‚ç‚¹å¯åŠ¨æƒ…å†µ", "è´¨é‡æŒ‡æ ‡å˜åŒ–", "ç”¨æˆ·åé¦ˆ"]
            }
        }
        
        debug_log(f"AIçŠ¶æ€æ›´æ–°å†³ç­–å®Œæˆï¼Œæ–°çŠ¶æ€: {new_state}")
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        error_result = {
            "success": False,
            "error": str(e),
            "error_type": "ai_decision_error",
            "node_id": node_id,
            "timestamp": datetime.datetime.now().isoformat()
        }
        debug_log(f"AIçŠ¶æ€æ›´æ–°å†³ç­–å¤±è´¥: {e}")
        return json.dumps(error_result, ensure_ascii=False, indent=2)


@mcp.tool()
async def ai_orchestrate_execution(
    dag_data: Annotated[str, Field(description="4å±‚DAGæ•°æ®JSONå­—ç¬¦ä¸²")] = "",
    execution_config: Annotated[str, Field(description="æ‰§è¡Œé…ç½®JSONå­—ç¬¦ä¸²ï¼ŒåŒ…å«åå¥½è®¾ç½®ã€çº¦æŸæ¡ä»¶ç­‰")] = "",
    user_preferences: Annotated[str, Field(description="ç”¨æˆ·åå¥½è®¾ç½®ï¼Œå¦‚åé¦ˆé¢‘ç‡ã€å†³ç­–å‚ä¸åº¦ç­‰")] = "",
) -> str:
    """
    AIæ™ºèƒ½ç¼–æ’4å±‚DAGæ‰§è¡Œæµç¨‹
    
    æ­¤å·¥å…·æ˜¯é«˜çº§ç¼–æ’å™¨ï¼Œæ•´åˆå››å¤§AI Agentçš„æ™ºèƒ½å†³ç­–ï¼Œç®¡ç†å®Œæ•´çš„DAGæ‰§è¡Œç”Ÿå‘½å‘¨æœŸã€‚
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. æ•´åˆå››å¤§AI Agentçš„æ™ºèƒ½å†³ç­–
    2. ç®¡ç†æ‰§è¡Œç”Ÿå‘½å‘¨æœŸ
    3. å¤„ç†å¼‚å¸¸å’Œå˜æ›´
    4. æä¾›å®æ—¶æ‰§è¡ŒæŠ¥å‘Š
    
    ç¼–æ’ç­–ç•¥ï¼š
    - æ™ºèƒ½è°ƒåº¦å’Œèµ„æºåˆ†é…
    - åŠ¨æ€è°ƒæ•´æ‰§è¡Œç­–ç•¥
    - å¼‚å¸¸æ£€æµ‹å’Œæ¢å¤
    - æŒç»­ä¼˜åŒ–å’Œå­¦ä¹ 
    
    Args:
        dag_data: å®Œæ•´çš„4å±‚DAGç»“æ„æ•°æ®
        execution_config: æ‰§è¡Œé…ç½®ï¼ŒåŒ…å«å„ç§è®¾ç½®å’Œçº¦æŸ
        user_preferences: ç”¨æˆ·åå¥½ï¼Œå½±å“å†³ç­–ç­–ç•¥
        
    Returns:
        str: JSONæ ¼å¼çš„æ‰§è¡Œç¼–æ’ç»“æœå’Œè®¡åˆ’
    """
    try:
        debug_log("å¼€å§‹AIæ™ºèƒ½æ‰§è¡Œç¼–æ’")
        
        # æ¨¡æ‹Ÿæ™ºèƒ½ç¼–æ’é€»è¾‘
        import json
        
        try:
            dag_info = json.loads(dag_data) if dag_data else {}
            config_info = json.loads(execution_config) if execution_config else {}
            preferences = json.loads(user_preferences) if user_preferences else {}
        except:
            dag_info = {}
            config_info = {}
            preferences = {}
        
        # åŸºç¡€ç¼–æ’ç­–ç•¥
        result = {
            "success": True,
            "orchestration_type": "ai_intelligent_execution",
            "timestamp": datetime.datetime.now().isoformat(),
            "execution_plan": {
                "phases": [
                    {
                        "phase_name": "blueprint_construction",
                        "description": "4å±‚DAGè“å›¾æ„å»ºé˜¶æ®µ",
                        "estimated_duration": "2-4å°æ—¶",
                        "ai_agents_involved": ["position_agent", "completion_agent", "next_node_agent", "update_agent"]
                    },
                    {
                        "phase_name": "validation_and_optimization",
                        "description": "éªŒè¯å’Œä¼˜åŒ–é˜¶æ®µ",
                        "estimated_duration": "1-2å°æ—¶",
                        "ai_agents_involved": ["completion_agent", "update_agent"]
                    },
                    {
                        "phase_name": "execution_monitoring",
                        "description": "æ‰§è¡Œç›‘æ§é˜¶æ®µ",
                        "estimated_duration": "æŒç»­",
                        "ai_agents_involved": ["position_agent", "next_node_agent"]
                    }
                ],
                "feedback_frequency": preferences.get("feedback_frequency", 3),
                "user_involvement_level": preferences.get("involvement_level", "moderate")
            },
            "intelligent_scheduling": {
                "current_strategy": "adaptive_priority_based",
                "scheduling_factors": ["ä¾èµ–å…³ç³»", "èµ„æºå¯ç”¨æ€§", "ä¸šåŠ¡ä¼˜å…ˆçº§", "é£é™©è¯„ä¼°"],
                "optimization_goals": ["æœ€çŸ­è·¯å¾„", "æœ€é«˜è´¨é‡", "æœ€ä½é£é™©"],
                "adaptation_triggers": ["ç”¨æˆ·åé¦ˆ", "æ‰§è¡Œå¼‚å¸¸", "èµ„æºå˜åŒ–"]
            },
            "monitoring_and_control": {
                "real_time_metrics": ["æ‰§è¡Œè¿›åº¦", "è´¨é‡æŒ‡æ ‡", "èµ„æºä½¿ç”¨", "é£é™©æ°´å¹³"],
                "alert_conditions": ["èŠ‚ç‚¹é˜»å¡", "è´¨é‡ä¸‹é™", "ç”¨æˆ·å¹²é¢„éœ€æ±‚"],
                "auto_recovery_actions": ["é‡æ–°è¯„ä¼°", "è°ƒæ•´ç­–ç•¥", "è¯·æ±‚ç”¨æˆ·ç¡®è®¤"]
            },
            "ai_decision_integration": {
                "position_identification": "å®æ—¶è¿è¡Œ",
                "completion_evaluation": "èŠ‚ç‚¹å®Œæˆæ—¶è§¦å‘",
                "next_node_recommendation": "çŠ¶æ€æ›´æ–°åè§¦å‘",
                "state_update_decision": "å®Œæˆè¯„ä¼°åè§¦å‘"
            },
            "user_interaction_plan": {
                "scheduled_feedback_points": ["é˜¶æ®µå®Œæˆæ—¶", "å…³é”®å†³ç­–ç‚¹", "å¼‚å¸¸å‘ç”Ÿæ—¶"],
                "feedback_collection_method": "mcp_feedback_enhanced_webui",
                "decision_escalation": "å¤æ‚æƒ…å†µè‡ªåŠ¨å‡çº§åˆ°ç”¨æˆ·"
            }
        }
        
        debug_log("AIæ™ºèƒ½æ‰§è¡Œç¼–æ’è®¡åˆ’ç”Ÿæˆå®Œæˆ")
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        error_result = {
            "success": False,
            "error": str(e),
            "error_type": "ai_orchestration_error",
            "timestamp": datetime.datetime.now().isoformat()
        }
        debug_log(f"AIæ™ºèƒ½æ‰§è¡Œç¼–æ’å¤±è´¥: {e}")
        return json.dumps(error_result, ensure_ascii=False, indent=2)


# ===== ä¸»ç¨‹å¼å…¥å£ =====
def main():
    """ä¸»è¦å…¥å£é»ï¼Œç”¨æ–¼å¥—ä»¶åŸ·è¡Œ"""
    # æª¢æŸ¥æ˜¯å¦å•Ÿç”¨èª¿è©¦æ¨¡å¼
    debug_enabled = os.getenv("MCP_DEBUG", "").lower() in ("true", "1", "yes", "on")

    # æª¢æŸ¥æ˜¯å¦å•Ÿç”¨æ¡Œé¢æ¨¡å¼
    desktop_mode = os.getenv("MCP_DESKTOP_MODE", "").lower() in (
        "true",
        "1",
        "yes",
        "on",
    )

    if debug_enabled:
        debug_log("ğŸš€ å•Ÿå‹•äº’å‹•å¼å›é¥‹æ”¶é›† MCP æœå‹™å™¨")
        debug_log(f"   æœå‹™å™¨åç¨±: {SERVER_NAME}")
        debug_log(f"   ç‰ˆæœ¬: {__version__}")
        debug_log(f"   å¹³å°: {sys.platform}")
        debug_log(f"   ç·¨ç¢¼åˆå§‹åŒ–: {'æˆåŠŸ' if _encoding_initialized else 'å¤±æ•—'}")
        debug_log(f"   é ç«¯ç’°å¢ƒ: {is_remote_environment()}")
        debug_log(f"   WSL ç’°å¢ƒ: {is_wsl_environment()}")
        debug_log(f"   æ¡Œé¢æ¨¡å¼: {'å•Ÿç”¨' if desktop_mode else 'ç¦ç”¨'}")
        debug_log("   ä»‹é¢é¡å‹: Web UI")
        debug_log("   ç­‰å¾…ä¾†è‡ª AI åŠ©æ‰‹çš„èª¿ç”¨...")
        debug_log("æº–å‚™å•Ÿå‹• MCP ä¼ºæœå™¨...")
        debug_log("èª¿ç”¨ mcp.run()...")

    try:
        # ä½¿ç”¨æ­£ç¢ºçš„ FastMCP API
        mcp.run()
    except KeyboardInterrupt:
        if debug_enabled:
            debug_log("æ”¶åˆ°ä¸­æ–·ä¿¡è™Ÿï¼Œæ­£å¸¸é€€å‡º")
        sys.exit(0)
    except Exception as e:
        if debug_enabled:
            debug_log(f"MCP æœå‹™å™¨å•Ÿå‹•å¤±æ•—: {e}")
            import traceback

            debug_log(f"è©³ç´°éŒ¯èª¤: {traceback.format_exc()}")
        sys.exit(1)


if __name__ == "__main__":
    main()
